{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F9tyhZxdfmZ"
      },
      "source": [
        "# LMKR RAG Chatbot - Jupyter Notebook\n",
        "\n",
        "This notebook implements a production-ready RAG (Retrieval-Augmented Generation) chatbot for LMKR using LangChain, HuggingFace LLM, and a custom vector database.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpZJ38pOdfma"
      },
      "source": [
        "## Cell 1: Project Setup & Dependencies Installation\n",
        "\n",
        "Install all required packages for LangChain, HuggingFace, FAISS, and utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDZf81Ozdfmb",
        "outputId": "92913036-7629-4c8d-d515-3d806dae7cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Collecting huggingface-hub\n",
            "  Using cached huggingface_hub-1.1.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub) (1.3.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub) (8.3.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install -U langchain langchain-community langchain-huggingface langchain-text-splitters\n",
        "!pip install -U huggingface-hub transformers torch\n",
        "!pip install sentence-transformers hf_xet\n",
        "!pip install faiss-cpu\n",
        "!pip install python-dotenv\n",
        "!pip install pypdf requests\n",
        "!pip install accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geMb4v3edfmb"
      },
      "source": [
        "**What this cell does:**\n",
        "- Installs LangChain and integration libraries\n",
        "- Installs HuggingFace models and utilities\n",
        "- Sets up FAISS (Facebook AI Similarity Search) for vector database\n",
        "- Installs data loading utilities (PDF parsing, web scraping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXRQ32wTdfmb"
      },
      "source": [
        "## Cell 2: Imports & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QKS9_Tztdfmb"
      },
      "outputs": [],
      "source": [
        "# ---- Standard Libraries ----\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ---- HuggingFace (LangChain Integration) ----\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import (\n",
        "    HuggingFaceEmbeddings,\n",
        "    HuggingFacePipeline\n",
        ")\n",
        "\n",
        "# ---- LangChain Core ----\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# ---- Document Loaders & Vectorstores (Community packages) ----\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# ---- RAG / Retrieval (no RetrievalQA in v1.x, use LCEL) ----\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# ---- LLMs ----\n",
        "from langchain_community.llms import HuggingFaceHub  # if you use HF hub models\n",
        "# or:\n",
        "# from langchain_community.chat_models import ChatOpenAI, ChatAnthropic, etc.\n",
        "\n",
        "# ---- Utility ----\n",
        "import torch\n",
        "import json\n",
        "from typing import Dict, List, Dict\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PesZGEC6dfmc"
      },
      "source": [
        "**What this cell does:**\n",
        "- Imports all necessary libraries from LangChain, HuggingFace, and transformers\n",
        "- Sets up logging for debugging and monitoring\n",
        "- Loads environment variables (useful for API keys or configurations)\n",
        "- Prepares utilities for text splitting, embeddings, and vector storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O56_GZL8dfmc"
      },
      "source": [
        "## Cell 3: Configuration Parameters\n",
        "\n",
        "Centralized configuration for the entire RAG pipeline. Tune these to optimize chatbot performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAewiAhadfmc",
        "outputId": "9a693c6d-b5d2-4c1d-8604-f130facaeb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Configuration loaded: LMKR-RAG-Chatbot\n",
            "âœ“ Device: Tesla T4\n",
            "âœ“ Results directory: ./results/20251125_120712\n"
          ]
        }
      ],
      "source": [
        "DATA_SOURCE_TYPE = \"text\"  # Options: \"text\", \"csv\", \"pdf\", \"url\"\n",
        "DATA_PATH = \"./Data/lmkr_data/lmkr_combined.txt\"  # Path to your LMKR data file\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 500\n",
        "\n",
        "# 3. EMBEDDING MODEL CONFIG\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "MODEL_KWARGS = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
        "ENCODE_KWARGS = {\"normalize_embeddings\": False}\n",
        "\n",
        "# 4. VECTOR DATABASE CONFIG\n",
        "VECTOR_DB_PATH = \"./vector_db/faiss_lmkr\"\n",
        "USE_EXISTING_DB = False\n",
        "\n",
        "# 5. LLM MODEL CONFIG\n",
        "LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "LLM_MAX_LENGTH = 100\n",
        "LLM_TEMPERATURE = 0.1\n",
        "LLM_TOP_P = 0.9\n",
        "\n",
        "\n",
        "\n",
        "RETRIEVER_K = 1\n",
        "CHAIN_TYPE = \"stuff\"\n",
        "\n",
        "# 7. PROJECT METADATA\n",
        "PROJECT_NAME = \"LMKR-RAG-Chatbot\"\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_DIR = f\"./results/{TIMESTAMP}\"\n",
        "\n",
        "print(f\"âœ“ Configuration loaded: {PROJECT_NAME}\")\n",
        "print(f\"âœ“ Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"âœ“ Results directory: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWHkcNq1dfmc"
      },
      "source": [
        "**What this cell does:**\n",
        "- Centralizes all hyperparameters and configurations\n",
        "- Explains trade-offs for each parameter\n",
        "- Makes it easy to experiment with different models and settings\n",
        "- Provides alternative model suggestions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkaMk5j6dfmd"
      },
      "source": [
        "## Cell 4: Data Loading & Exploration\n",
        "\n",
        "Load your LMKR data from various sources (text, CSV, PDF, web)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNVb5iVVdfmd",
        "outputId": "3da6c74f-9e6f-441f-9038-10fbb6ecf6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Successfully loaded LMKR data\n",
            "  - Total characters: 5,903\n",
            "  - Estimated tokens: 1,475\n",
            "  - Ready for RAG pipeline\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD LMKR DATA\n",
        "# Simple and clean - just raw text, no URLs\n",
        "# ============================================================\n",
        "\n",
        "try:\n",
        "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    print(f\"âœ“ Successfully loaded LMKR data\")\n",
        "    print(f\"  - Total characters: {len(raw_data):,}\")\n",
        "    print(f\"  - Estimated tokens: {len(raw_data) // 4:,}\")\n",
        "    print(f\"  - Ready for RAG pipeline\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ File not found: {DATA_PATH}\")\n",
        "    print(f\"Please ensure lmkr_combined.txt exists in ./data/lmkr_data/\")\n",
        "    raw_data = \"\"\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading file: {e}\")\n",
        "    raw_data = \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi5cqmiodfmd"
      },
      "source": [
        "**What this cell does:**\n",
        "- Loads LMKR data from various sources (text, CSV, PDF)\n",
        "- Handles different data formats automatically\n",
        "- Provides basic data statistics\n",
        "- Shows preview to verify data loaded correctly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkRO_HuSdfmd"
      },
      "source": [
        "## Cell 5: Data Preprocessing & Text Chunking\n",
        "\n",
        "Clean, normalize, and split text into manageable chunks for embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku8-W0Ekdfmd",
        "outputId": "cc3fed67-787e-433c-8ce9-dbcacd2cdc4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text...\n",
            "Chunking text...\n",
            "\\nâœ“ Chunking Statistics:\n",
            "  - Total chunks: 11\n",
            "  - Average chunk size: 986 chars\n",
            "  - Max chunk size: 998 chars\n",
            "  - Min chunk size: 912 chars\n",
            "\\nFirst chunk preview:\\n# LMKR Company Data Collection & Storage Guide\n",
            "\n",
            "## ðŸ“Š COMPREHENSIVE LMKR COMPANY INFORMATION\n",
            "\n",
            "### **Company Overview**\n",
            "- **Name:** LMKR\n",
            "- **Founded:** 1994\n",
            "- **Headquarters:** Houston, Texas\n",
            "- **Type:** Privately Held Technology Company\n",
            "- **Employees:** 201-500\n",
            "- **Industry:** Oil & Gas, Geoscience, ...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PREPROCESS DATA & CREATE CHUNKS\n",
        "# This prepares raw text for embedding and retrieval\n",
        "# ============================================================\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean and preprocess text.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    text = re.sub(r'\\\\n+', '\\\\n', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into chunks using RecursiveCharacterTextSplitter.\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=overlap,\n",
        "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\n",
        "    )\n",
        "    chunks = splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# Execute preprocessing\n",
        "print(\"Preprocessing text...\")\n",
        "cleaned_data = preprocess_text(raw_data)\n",
        "\n",
        "print(\"Chunking text...\")\n",
        "text_chunks = chunk_text(cleaned_data, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "\n",
        "print(f\"\\\\nâœ“ Chunking Statistics:\")\n",
        "print(f\"  - Total chunks: {len(text_chunks)}\")\n",
        "if text_chunks:\n",
        "    print(f\"  - Average chunk size: {sum(len(c) for c in text_chunks) // len(text_chunks)} chars\")\n",
        "    print(f\"  - Max chunk size: {max(len(c) for c in text_chunks)} chars\")\n",
        "    print(f\"  - Min chunk size: {min(len(c) for c in text_chunks)} chars\")\n",
        "    print(f\"\\\\nFirst chunk preview:\\\\n{text_chunks[0][:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99VqB0O1dfmd"
      },
      "source": [
        "**What this cell does:**\n",
        "- Cleans and normalizes text\n",
        "- Splits text into manageable chunks (default 1000 chars)\n",
        "- Maintains overlap for context continuity\n",
        "- Provides statistics on chunking effectiveness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDOgy3Szdfmd"
      },
      "source": [
        "## Cell 6: Embedding Model Setup\n",
        "\n",
        "Initialize embedding model to convert text into vector representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG0l9T8Fdfme",
        "outputId": "849f4946-7222-4cb5-b6e6-d98ca80e2a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\nâœ“ Embedding test successful!\n",
            "  - Sample query embedding shape: 384\n",
            "  - First 5 values: [0.052791938185691833, 0.0005524340667761862, -0.05132013186812401, 0.07101751118898392, 0.03979623317718506]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# INITIALIZE EMBEDDING MODEL\n",
        "# This converts text into vector representations\n",
        "# ============================================================\n",
        "\n",
        "print(\"Loading embedding model...\")\n",
        "print(f\"Model: {EMBEDDING_MODEL_NAME}\")\n",
        "print(f\"Device: {MODEL_KWARGS['device']}\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs=MODEL_KWARGS,\n",
        "    encode_kwargs=ENCODE_KWARGS\n",
        ")\n",
        "\n",
        "# Test embedding on a sample\n",
        "test_text = \"LMKR is a test query\"\n",
        "test_embedding = embeddings.embed_query(test_text)\n",
        "print(f\"\\\\nâœ“ Embedding test successful!\")\n",
        "print(f\"  - Sample query embedding shape: {len(test_embedding)}\")\n",
        "print(f\"  - First 5 values: {test_embedding[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJZLFuWndfme"
      },
      "source": [
        "**What this cell does:**\n",
        "- Loads embedding model from HuggingFace\n",
        "- Initializes embeddings for your GPU/CPU\n",
        "- Tests embedding functionality\n",
        "- Explains model trade-offs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWo19pWBdfme"
      },
      "source": [
        "## Cell 7: Vector Database Creation & Indexing\n",
        "\n",
        "Convert text chunks to embeddings and store in FAISS for fast retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_BAO94kdfme",
        "outputId": "2936c71f-98bf-42ad-89d6-ad97a5f3279f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vector database...\n",
            "Number of documents to embed: 11\n",
            "âœ“ Vector database created and saved to ./vector_db/faiss_lmkr\n",
            "\\n--- Testing Retriever ---\n",
            "Retrieved 1 documents for query: 'What is LMKR?'\n",
            "\\n[Document 1]\n",
            "Content: # LMKR Company Data Collection & Storage Guide\n",
            "\n",
            "## ðŸ“Š COMPREHENSIVE LMKR COMPANY INFORMATION\n",
            "\n",
            "### **Company Overview**\n",
            "- **Name:** LMKR\n",
            "- **Founded:** 1994\n",
            "- **Headquarters:** Houston, Texas\n",
            "- **Type:*...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CREATE & INDEX VECTOR DATABASE\n",
        "# Converts text chunks to embeddings and stores in FAISS\n",
        "# ============================================================\n",
        "\n",
        "def create_vector_db(documents: List[str], embeddings, db_path: str):\n",
        "    \"\"\"\n",
        "    Create FAISS vector database from documents.\n",
        "    \"\"\"\n",
        "    print(\"Creating vector database...\")\n",
        "    print(f\"Number of documents to embed: {len(documents)}\")\n",
        "\n",
        "    vector_db = FAISS.from_texts(\n",
        "        texts=documents,\n",
        "        embedding=embeddings,\n",
        "        metadatas=[{\"source\": f\"chunk_{i}\"} for i in range(len(documents))]\n",
        "    )\n",
        "\n",
        "    os.makedirs(db_path, exist_ok=True)\n",
        "    vector_db.save_local(db_path)\n",
        "    print(f\"âœ“ Vector database created and saved to {db_path}\")\n",
        "    return vector_db\n",
        "\n",
        "def load_vector_db(embeddings, db_path: str):\n",
        "    \"\"\"\n",
        "    Load existing FAISS vector database.\n",
        "    \"\"\"\n",
        "    print(f\"Loading vector database from {db_path}...\")\n",
        "    vector_db = FAISS.load_local(db_path, embeddings)\n",
        "    print(f\"âœ“ Vector database loaded successfully\")\n",
        "    return vector_db\n",
        "\n",
        "if USE_EXISTING_DB and os.path.exists(VECTOR_DB_PATH):\n",
        "    vector_store = load_vector_db(embeddings, VECTOR_DB_PATH)\n",
        "else:\n",
        "    vector_store = create_vector_db(text_chunks, embeddings, VECTOR_DB_PATH)\n",
        "\n",
        "print(\"\\\\n--- Testing Retriever ---\")\n",
        "test_query = \"What is LMKR?\"  # TODO: Modify based on domain\n",
        "retrieved_docs = vector_store.similarity_search(test_query, k=RETRIEVER_K)\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents for query: '{test_query}'\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\\\n[Document {i+1}]\")\n",
        "    print(f\"Content: {doc.page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlCyJ7TPdfme"
      },
      "source": [
        "**What this cell does:**\n",
        "- Converts all text chunks to embeddings\n",
        "- Creates FAISS index for fast similarity search\n",
        "- Saves index to disk for reuse\n",
        "- Tests retriever with a sample query\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J69JA0Y_dfme"
      },
      "source": [
        "## Cell 8: LLM Model Setup (HuggingFace)\n",
        "\n",
        "Load language model that will generate final responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "b5f16afd346e4c1e8b19bc42bd48ccb7",
            "c8ff811133fa4937b6d7dcce4785232a",
            "caf719147f6b412d9ce6228d0f8df60d",
            "20156f2be3ef44b581afe678af8b4ec4",
            "7adc0ec99a804d5ea23cffee733efa79",
            "fc3bd057cb654a11886af051d4513005",
            "6f3d50ad086548ba904fd1b1e50d2043",
            "eb10d0f7669c4050a4260b60554499ca",
            "f54165cf41594bb6939eb887c4fc39da",
            "4d3fa6fdcfa64a7b96688836ce6b48b4",
            "e2dbfc3449274c91ac8ca73dec1966d6"
          ]
        },
        "id": "ZsjJoXGmdfme",
        "outputId": "b2f040ed-d94f-4c41-bec9-079efdd42663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLM from HuggingFace...\n",
            "Model: mistralai/Mistral-7B-Instruct-v0.1\n",
            "Device: Tesla T4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5f16afd346e4c1e8b19bc42bd48ccb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model loaded successfully\n",
            "âœ“ LLM pipeline created successfully\n",
            "\\n--- Testing LLM ---\n",
            "Input: Tell me some of LMKR clients:\n",
            "Output: Tell me some of LMKR clients:\n",
            "\n",
            "1. ABB\n",
            "2. Siemens\n",
            "3. GE\n",
            "4. Honeywell\n",
            "5. Rockwell Automation\n",
            "6. Schneider Electric\n",
            "7. Emerson Electric\n",
            "8. Bosch Rexroth\n",
            "9. Mitsubishi Electric\n",
            "10. Ametek\n",
            "11. National Ins\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD LANGUAGE MODEL (HuggingFace)\n",
        "# This generates the final responses\n",
        "# ============================================================\n",
        "\n",
        "print(\"Loading LLM from HuggingFace...\")\n",
        "print(f\"Model: {LLM_MODEL_NAME}\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, max_length=LLM_MAX_LENGTH)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    )\n",
        "    print(\"âœ“ Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "# Create pipeline\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=LLM_MAX_LENGTH,\n",
        "    temperature=LLM_TEMPERATURE,\n",
        "    top_p=LLM_TOP_P,\n",
        ")\n",
        "\n",
        "# Wrap in LangChain\n",
        "llm = HuggingFacePipeline(\n",
        "    model_id=LLM_MODEL_NAME,\n",
        "    pipeline=llm_pipeline,\n",
        ")\n",
        "\n",
        "print(\"âœ“ LLM pipeline created successfully\")\n",
        "\n",
        "# Test LLM\n",
        "print(\"\\\\n--- Testing LLM ---\")\n",
        "test_prompt = \"Tell me some of LMKR clients:\"\n",
        "try:\n",
        "    test_output = llm.invoke(test_prompt)\n",
        "    print(f\"Input: {test_prompt}\")\n",
        "    print(f\"Output: {test_output[:200]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: LLM test skipped ({type(e).__name__})\")\n",
        "    print(f\"LLM is ready to use in RAG chain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uithjOL1dfme"
      },
      "source": [
        "**What this cell does:**\n",
        "- Loads tokenizer and model from HuggingFace\n",
        "- Configures GPU optimization\n",
        "- Creates text generation pipeline\n",
        "- Tests LLM functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJdEBdyYdfmf"
      },
      "source": [
        "## Cell 9: System Prompt Engineering\n",
        "\n",
        "Craft system prompts to guide chatbot behavior and response quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYSxmnqEdfmf",
        "outputId": "5c4d159d-4704-4f5d-f83d-23408c8bdbed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ System prompt configured for Mistral-7B\n",
            "âœ“ System prompts defined\n",
            "Active prompt: SYSTEM_PROMPT_CUSTOMER_SERVICE\n",
            "\\nPrompt preview:\\nAnswer this question in your own words based on the provided information. write bla bla before answering.\n",
            "Context:\n",
            "{context}\n",
            "\n",
            "Question: {question}\n",
            "\n",
            "Answer in 1-2 sentences:...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CRAFT SYSTEM PROMPTS FOR CHATBOT\n",
        "# This is critical for accuracy and response quality\n",
        "# ============================================================\n",
        "\n",
        "# Customer service oriented\n",
        "SYSTEM_PROMPT_TEMPLATE = \"\"\"Answer this question in your own words based on the provided information. write bla bla before answering.\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in 1-2 sentences:\"\"\"\n",
        "\n",
        "ACTIVE_SYSTEM_PROMPT = SYSTEM_PROMPT_TEMPLATE\n",
        "\n",
        "print(\"âœ“ System prompt configured for Mistral-7B\")\n",
        "\n",
        "print(\"âœ“ System prompts defined\")\n",
        "print(f\"Active prompt: SYSTEM_PROMPT_CUSTOMER_SERVICE\")\n",
        "print(f\"\\\\nPrompt preview:\\\\n{ACTIVE_SYSTEM_PROMPT[:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d-wbkFhdfmf"
      },
      "source": [
        "**What this cell does:**\n",
        "- Defines multiple system prompt templates\n",
        "- Explains different prompt strategies\n",
        "- Provides template for context-aware responses\n",
        "\n",
        "**Prompt Engineering Tips:**\n",
        "- Be specific about role and constraints\n",
        "- Provide clear instructions on using context\n",
        "- Include examples of desired behavior\n",
        "- Experiment and measure quality differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-3HUVndfmf"
      },
      "source": [
        "## Cell 10: RAG Chain Assembly\n",
        "\n",
        "Combine retriever, prompt template, and LLM into complete RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoPQPGKFdfmf",
        "outputId": "6b15fbba-8624-46a0-c4f2-68757ce13cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building RAG chain...\n",
            "âœ“ RAG chain assembled successfully\n",
            "Chain configuration:\n",
            "  - Retriever K: 1\n",
            "  - LLM: mistralai/Mistral-7B-Instruct-v0.1\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ASSEMBLE RAG CHAIN (MODERN LANGCHAIN APPROACH)\n",
        "# Combines retriever, prompt template, and LLM\n",
        "# ============================================================\n",
        "\n",
        "# Create prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=ACTIVE_SYSTEM_PROMPT\n",
        ")\n",
        "\n",
        "# Create retriever from vector store\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": RETRIEVER_K}\n",
        ")\n",
        "\n",
        "# Helper function to format documents\n",
        "def format_docs(docs):\n",
        "    \"\"\"Format retrieved documents for the prompt\"\"\"\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Assemble RAG chain using modern LCEL (LangChain Expression Language)\n",
        "print(\"Building RAG chain...\")\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"âœ“ RAG chain assembled successfully\")\n",
        "print(f\"Chain configuration:\")\n",
        "print(f\"  - Retriever K: {RETRIEVER_K}\")\n",
        "print(f\"  - LLM: {LLM_MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYPGKzBodfmf"
      },
      "source": [
        "**What this cell does:**\n",
        "- Creates prompt template for RAG\n",
        "- Converts vector store to retriever\n",
        "- Assembles RAG chain combining all components\n",
        "- Configures chain parameters\n",
        "\n",
        "**Chain Types:**\n",
        "- `stuff`: Concatenate docs (simple, works for small contexts)\n",
        "- `map_reduce`: Summarize each doc, then combine (for many docs)\n",
        "- `refine`: Iteratively improve answer (best accuracy, slower)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozy76mXDdfmf"
      },
      "source": [
        "## Cell 11: Testing & Inference\n",
        "\n",
        "Test RAG chain with sample queries and evaluate responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ318KeOdfmf",
        "outputId": "e7c2bcc1-1c0b-463d-92d5-4a2c57ba1f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TESTING RAG CHATBOT\n",
            "======================================================================\n",
            "\n",
            "[Query 1/4]\n",
            "Question: When was LMKR founded?\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LMKR was founded in 1994.\n",
            "\n",
            "[Query 2/4]\n",
            "Question: What is Gverse?\n",
            "----------------------------------------------------------------------\n",
            "Answer: Gverse is a leading-edge geoscience software platform used for exploration and production in the oil and gas industry. It offers a range of features including mapping and geological interpretation, geophysical analysis, petrophysical interpretation, structural modeling, and well and field planning.\n",
            "\n",
            "[Query 3/4]\n",
            "Question: What are LMKR's ISO-Certified Services?\n",
            "----------------------------------------------------------------------\n",
            "Answer: LMKR offers a range of ISO-certified services, including geoscience interpretation, geophysical analysis, petrophysical interpretation, structural modeling, well and field planning, and real-time seismic attribute analysis.\n",
            "\n",
            "[Query 4/4]\n",
            "Question: Name one of LMKR's partnership?\n",
            "----------------------------------------------------------------------\n",
            "Answer: LMKR has partnerships with several companies in the oil and gas industry, including Halliburton, Schlumberger, and Baker Hughes.\n",
            "\n",
            "âœ“ Test results saved to ./results/20251125_120712/test_results.json\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# TEST RAG CHATBOT\n",
        "# Run queries and evaluate responses\n",
        "# ============================================================\n",
        "\n",
        "def query_chatbot(rag_chain, retriever, query: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Query the RAG chatbot and return result with sources.\n",
        "    \"\"\"\n",
        "    docs = retriever.invoke(query)\n",
        "    answer = rag_chain.invoke(query)\n",
        "\n",
        "    # Extract only the answer portion (remove prompt)\n",
        "    if \"Answer in 1-2 sentences:\" in answer:\n",
        "        answer = answer.split(\"Answer in 1-2 sentences:\")[-1].strip()\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"source_documents\": docs,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "\n",
        "test_queries = [\n",
        "    \"When was LMKR founded?\",\n",
        "    \"What is Gverse?\",\n",
        "    \"What are LMKR's ISO-Certified Services?\",\n",
        "    \"Name one of LMKR's partnership?\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING RAG CHATBOT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for i, query in enumerate(test_queries):\n",
        "    print(f\"\\n[Query {i+1}/{len(test_queries)}]\")\n",
        "    print(f\"Question: {query}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    try:\n",
        "        result = query_chatbot(rag_chain, retriever, query)  # Pass retriever\n",
        "        test_results.append(result)\n",
        "\n",
        "        print(f\"Answer: {result['answer']}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing query: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "# Save test results\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "results_path = f\"{RESULTS_DIR}/test_results.json\"\n",
        "\n",
        "# Convert Document objects to dictionaries for JSON serialization\n",
        "serializable_results = []\n",
        "for result in test_results:\n",
        "    serializable_result = {\n",
        "        \"query\": result[\"query\"],\n",
        "        \"answer\": result[\"answer\"],\n",
        "        \"source_documents\": [\n",
        "            {\n",
        "                \"content\": doc.page_content,\n",
        "                \"metadata\": doc.metadata\n",
        "            }\n",
        "            for doc in result[\"source_documents\"]\n",
        "        ],\n",
        "        \"timestamp\": result[\"timestamp\"]\n",
        "    }\n",
        "    serializable_results.append(serializable_result)\n",
        "\n",
        "with open(results_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(serializable_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nâœ“ Test results saved to {results_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuxZ6XY2dfmg"
      },
      "source": [
        "**What this cell does:**\n",
        "- Tests RAG chain with sample queries\n",
        "- Captures answers and source documents\n",
        "- Shows retrieval effectiveness\n",
        "- Saves results for analysis\n",
        "\n",
        "**Next Steps After Testing:**\n",
        "- Evaluate answer quality\n",
        "- Check if correct documents were retrieved\n",
        "- Refine prompts if needed\n",
        "- Adjust retriever K if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF5Zinmadfmg"
      },
      "source": [
        "## Cell 12: Interactive Chat Loop\n",
        "\n",
        "Enable interactive chat with continuous conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Gcte64dfmg",
        "outputId": "a60db22d-ff05-4a20-cb98-d12328be4c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LMKR RAG CHATBOT - Interactive Mode\n",
            "Type 'quit' to exit, 'clear' to clear history\n",
            "======================================================================\n",
            "\\nYou: hi\n",
            "Error: query_chatbot() missing 1 required positional argument: 'query'\n",
            "\\nYou: hello\n",
            "Error: query_chatbot() missing 1 required positional argument: 'query'\n",
            "\\nYou: LMKR\n",
            "Error: query_chatbot() missing 1 required positional argument: 'query'\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# INTERACTIVE CHATBOT\n",
        "# Allows continuous conversation with the chatbot\n",
        "# ============================================================\n",
        "\n",
        "def chat_with_bot(rag_chain, history: List = None):\n",
        "    \"\"\"\n",
        "    Interactive chat interface for the RAG chatbot.\n",
        "    Type 'quit' to exit.\n",
        "    \"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"LMKR RAG CHATBOT - Interactive Mode\")\n",
        "    print(\"Type 'quit' to exit, 'clear' to clear history\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() == \"quit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input.lower() == \"clear\":\n",
        "            history = []\n",
        "            print(\"History cleared\")\n",
        "            continue\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            result = query_chatbot(rag_chain, user_input)\n",
        "            history.append(result)\n",
        "            print(f\"\\\\nBot: {result['answer']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# TODO: Uncomment to run interactive chat\n",
        "chat_history = chat_with_bot(rag_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H26nFS0idfmg"
      },
      "source": [
        "**What this cell does:**\n",
        "- Provides interactive chat interface\n",
        "- Maintains conversation history\n",
        "- Shows bot responses in real-time\n",
        "\n",
        "**Usage:**\n",
        "- Uncomment the last line to activate\n",
        "- Type your questions\n",
        "- Type 'quit' to exit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2SmBMSjdfmg"
      },
      "source": [
        "## Cell 13: Evaluation & Metrics\n",
        "\n",
        "Measure and track chatbot quality metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHpC-kxkdfmg",
        "outputId": "c6d63e5e-7793-4cb5-b69f-e7ce447bc37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\n======================================================================\n",
            "EVALUATION METRICS\n",
            "======================================================================\n",
            "total_queries: 4\n",
            "avg_answer_length: 196\n",
            "avg_sources_retrieved: 1.0\n",
            "queries_with_sources: 4\n",
            "\\nâœ“ Metrics saved to ./results/20251125_120712/metrics.json\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# EVALUATE CHATBOT QUALITY\n",
        "# Measure accuracy, relevance, and response quality\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_responses(test_results: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate chatbot response quality.\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        \"total_queries\": len(test_results),\n",
        "        \"avg_answer_length\": 0,\n",
        "        \"avg_sources_retrieved\": 0,\n",
        "        \"queries_with_sources\": 0,\n",
        "    }\n",
        "\n",
        "    if not test_results:\n",
        "        return metrics\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_length = sum(len(r[\"answer\"]) for r in test_results)\n",
        "    total_sources = sum(len(r[\"source_documents\"]) for r in test_results)\n",
        "\n",
        "    metrics[\"avg_answer_length\"] = total_length // len(test_results)\n",
        "    metrics[\"avg_sources_retrieved\"] = total_sources / len(test_results)\n",
        "    metrics[\"queries_with_sources\"] = sum(\n",
        "        1 for r in test_results if len(r[\"source_documents\"]) > 0\n",
        "    )\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Evaluate test results\n",
        "if test_results:\n",
        "    metrics = evaluate_responses(test_results)\n",
        "    print(\"\\\\n\" + \"=\" * 70)\n",
        "    print(\"EVALUATION METRICS\")\n",
        "    print(\"=\" * 70)\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_path = f\"{RESULTS_DIR}/metrics.json\"\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    print(f\"\\\\nâœ“ Metrics saved to {metrics_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbtNJX1Jdfmk"
      },
      "source": [
        "**What this cell does:**\n",
        "- Calculates basic quality metrics\n",
        "- Provides framework for custom evaluation\n",
        "- Saves metrics for comparison\n",
        "\n",
        "**Evaluation Framework:**\n",
        "- Manual scoring: Rate 1-5 (relevance, accuracy, clarity)\n",
        "- Automated metrics: BLEU, ROUGE, similarity scores\n",
        "- Comparison: Test different models/prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA41TmIzdfml"
      },
      "source": [
        "## Cell 14: Save & Deploy Configuration\n",
        "\n",
        "Save all settings for reproducibility and future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0lCPltBdfml",
        "outputId": "ad916401-3a5d-411c-96f3-b51188a33846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Configuration saved to ./results/20251125_120712/config.json\n",
            "\\n======================================================================\n",
            "PROJECT SUMMARY\n",
            "======================================================================\n",
            "Project: LMKR-RAG-Chatbot\n",
            "Timestamp: 20251125_120712\n",
            "Results directory: ./results/20251125_120712\n",
            "\\nKey Components:\n",
            "  - Data chunks: 11\n",
            "  - Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "  - LLM: mistralai/Mistral-7B-Instruct-v0.1\n",
            "  - Retriever K: 1\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE PROJECT CONFIGURATION\n",
        "# Store settings for reproducibility\n",
        "# ============================================================\n",
        "\n",
        "def save_project_config():\n",
        "    \"\"\"\n",
        "    Save all project configuration to JSON.\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        \"project_name\": PROJECT_NAME,\n",
        "        \"timestamp\": TIMESTAMP,\n",
        "        \"data\": {\n",
        "            \"source_type\": DATA_SOURCE_TYPE,\n",
        "            \"path\": DATA_PATH,\n",
        "            \"num_chunks\": len(text_chunks) if text_chunks else 0,\n",
        "            \"chunk_size\": CHUNK_SIZE,\n",
        "            \"chunk_overlap\": CHUNK_OVERLAP,\n",
        "        },\n",
        "        \"embedding\": {\n",
        "            \"model\": EMBEDDING_MODEL_NAME,\n",
        "            \"device\": MODEL_KWARGS[\"device\"],\n",
        "        },\n",
        "        \"vectordb\": {\n",
        "            \"path\": VECTOR_DB_PATH,\n",
        "            \"retriever_k\": RETRIEVER_K,\n",
        "        },\n",
        "        \"llm\": {\n",
        "            \"model\": LLM_MODEL_NAME,\n",
        "            \"max_length\": LLM_MAX_LENGTH,\n",
        "            \"temperature\": LLM_TEMPERATURE,\n",
        "            \"top_p\": LLM_TOP_P,\n",
        "        },\n",
        "        \"rag_chain\": {\n",
        "            \"chain_type\": CHAIN_TYPE,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    config_path = f\"{RESULTS_DIR}/config.json\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"âœ“ Configuration saved to {config_path}\")\n",
        "    return config\n",
        "\n",
        "# Save configuration\n",
        "config = save_project_config()\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Project: {config['project_name']}\")\n",
        "print(f\"Timestamp: {config['timestamp']}\")\n",
        "print(f\"Results directory: {RESULTS_DIR}\")\n",
        "print(f\"\\\\nKey Components:\")\n",
        "print(f\"  - Data chunks: {config['data']['num_chunks']}\")\n",
        "print(f\"  - Embedding model: {config['embedding']['model']}\")\n",
        "print(f\"  - LLM: {config['llm']['model']}\")\n",
        "print(f\"  - Retriever K: {config['vectordb']['retriever_k']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGdHVNaydfml"
      },
      "source": [
        "**What this cell does:**\n",
        "- Saves all configurations for reproducibility\n",
        "- Documents experiment settings\n",
        "- Creates project summary\n",
        "\n",
        "**Why It Matters:**\n",
        "- Enables reproducible experiments\n",
        "- Documents what settings worked best\n",
        "- Useful for sharing with teammates\n",
        "- Easy to restart/continue work"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5f16afd346e4c1e8b19bc42bd48ccb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8ff811133fa4937b6d7dcce4785232a",
              "IPY_MODEL_caf719147f6b412d9ce6228d0f8df60d",
              "IPY_MODEL_20156f2be3ef44b581afe678af8b4ec4"
            ],
            "layout": "IPY_MODEL_7adc0ec99a804d5ea23cffee733efa79"
          }
        },
        "c8ff811133fa4937b6d7dcce4785232a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc3bd057cb654a11886af051d4513005",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6f3d50ad086548ba904fd1b1e50d2043",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "caf719147f6b412d9ce6228d0f8df60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb10d0f7669c4050a4260b60554499ca",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f54165cf41594bb6939eb887c4fc39da",
            "value": 2
          }
        },
        "20156f2be3ef44b581afe678af8b4ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d3fa6fdcfa64a7b96688836ce6b48b4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e2dbfc3449274c91ac8ca73dec1966d6",
            "value": "â€‡2/2â€‡[01:38&lt;00:00,â€‡45.63s/it]"
          }
        },
        "7adc0ec99a804d5ea23cffee733efa79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc3bd057cb654a11886af051d4513005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3d50ad086548ba904fd1b1e50d2043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb10d0f7669c4050a4260b60554499ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f54165cf41594bb6939eb887c4fc39da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d3fa6fdcfa64a7b96688836ce6b48b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2dbfc3449274c91ac8ca73dec1966d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}