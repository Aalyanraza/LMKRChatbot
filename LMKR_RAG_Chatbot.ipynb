{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Data Structures\n",
    "We define strict Pydantic models for every node's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarkazmi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Optional, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup & Configuration (Kept Exact) ---\n",
    "load_dotenv()\n",
    "\n",
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Vector DB\n",
    "VECTOR_DB_PATH = \"./vector_db/faiss_lmkr\"\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "except:\n",
    "    print(\"âš ï¸ DB not found, creating dummy for execution safety.\")\n",
    "    vectorstore = FAISS.from_texts([\"LMKR founded in 1994. GVERSE is a software brand.\"], embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "# LLM Client\n",
    "hf_client = InferenceClient(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    token=os.getenv(\"HF_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# --- 2. Pydantic Models for Structured Output ---\n",
    "\n",
    "class QueryAugmentation(BaseModel):\n",
    "    \"\"\"Output for Node 1: Retrieval Augmentation\"\"\"\n",
    "    augmented_queries: List[str] = Field(\n",
    "        description=\"List of 3 alternative versions of the user question to improve search coverage.\"\n",
    "    )\n",
    "\n",
    "class GeneratedAnswer(BaseModel):\n",
    "    \"\"\"Output for Node 2: Generation\"\"\"\n",
    "    answer: str = Field(description=\"The response to the user.\")\n",
    "    sources_used: List[str] = Field(description=\"List of context chunks or titles used.\")\n",
    "\n",
    "    # FIX: Add this validator to handle cases where LLM returns a list for the answer\n",
    "    @field_validator('answer', mode='before')\n",
    "    @classmethod\n",
    "    def flatten_list_answer(cls, v):\n",
    "        if isinstance(v, list):\n",
    "            return \", \".join(map(str, v))\n",
    "        return v\n",
    "        \n",
    "class ValidationResult(BaseModel):\n",
    "    \"\"\"Output for Node 3: Validation\"\"\"\n",
    "    is_valid: bool = Field(description=\"True if context was used correctly and no hallucinations found.\")\n",
    "    reason: str = Field(description=\"Explanation of validation failure or success.\")\n",
    "\n",
    "# --- 3. State Definition ---\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    context_chunks: List[str]\n",
    "    generated_answer: Optional[GeneratedAnswer]\n",
    "    validation: Optional[ValidationResult]\n",
    "    retry_count: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Helper for Mistral JSON Enforcement\n",
    "Since Mistral can be chatty, this helper ensures we get clean JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_structured(prompt_text: str, parser: PydanticOutputParser) -> Optional[BaseModel]:\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    \n",
    "    # FIX 1: Stronger prompt to stop it from returning the Schema Definition\n",
    "    final_prompt = f\"\"\"{prompt_text}\n",
    "    \n",
    "    IMPORTANT INSTRUCTIONS:\n",
    "    1. Output ONLY a valid JSON object. \n",
    "    2. Do NOT output the schema definition or \"properties\" block. Output the actual data instance.\n",
    "    3. Do NOT escape underscores (e.g., use \"sources_used\", NOT \"sources\\\\_used\").\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": final_prompt}]\n",
    "        response = hf_client.chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        json_str = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean Markdown wrapping\n",
    "        if \"```json\" in json_str:\n",
    "            json_str = json_str.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in json_str:\n",
    "            json_str = json_str.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "        # FIX 2: Manually fix the \"sources\\_used\" error common in Mistral models\n",
    "        json_str = json_str.replace(r\"\\_\", \"_\")\n",
    "\n",
    "        # FIX 3: Detect if model returned a Schema instead of Data (Node 1 Fix)\n",
    "        # If the JSON looks like {\"properties\": {...}, \"type\": \"object\"}, it failed.\n",
    "        # We can try to salvage it or just return None to trigger a retry/fallback.\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            if \"properties\" in data and \"type\" in data and data.get(\"type\") == \"object\":\n",
    "                print(\"âš ï¸ Model returned schema instead of data. Retrying parse...\")\n",
    "                # Sometimes models put the answer inside 'default' or 'example' fields of the schema, \n",
    "                # but usually it's best to just fail and let the fallback handle it.\n",
    "                return None\n",
    "        except:\n",
    "            pass # Not valid JSON yet, let the parser handle the error\n",
    "            \n",
    "        return parser.parse(json_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JSON Parsing/API Failed: {e}\")\n",
    "        # print(f\"Raw: {json_str}\") # Uncomment for debugging\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define The Router and The 5 Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "# --- 1. Tool Extraction from scraping.py ---\n",
    "\n",
    "def get_soup(url: str):\n",
    "    \"\"\"Helper to get BeautifulSoup object safely.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, verify=False, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_and_clean_body(url: str) -> str:\n",
    "    soup = get_soup(url)\n",
    "    if not soup: return \"\"\n",
    "    \n",
    "    # Remove non-content tags\n",
    "    tags_to_remove = [\"nav\", \"footer\", \"script\", \"style\", \"noscript\", \"iframe\", \"svg\"]\n",
    "    for tag in soup(tags_to_remove):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Remove noise classes\n",
    "    noise_classes = [\"cookie\", \"popup\", \"newsletter\", \"signup\", \"login\", \"breadcrumb\", \"sidebar\"]\n",
    "    for noise in noise_classes:\n",
    "        for element in soup.find_all(class_=re.compile(noise, re.IGNORECASE)):\n",
    "            element.decompose()\n",
    "\n",
    "    # Extract main content\n",
    "    main_content = soup.find('main') or soup.find('article') or soup.find(id='content')\n",
    "    if main_content:\n",
    "        text = main_content.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "    return text\n",
    "\n",
    "def clean_text_content(text: str) -> str:\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    NOISE_PHRASES = [\"warning\", \"required\", \"skip to content\", \"all rights reserved\"]\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if len(stripped) < 3: continue\n",
    "        if any(phrase in stripped.lower() for phrase in NOISE_PHRASES): continue\n",
    "        cleaned_lines.append(stripped)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def scrape_contact_tool():\n",
    "    \"\"\"Scrapes LMKR contact page and saves to file.\"\"\"\n",
    "    url = \"https://lmkr.com/contact/\"\n",
    "    print(f\"ðŸ•¸ï¸ Tool Triggered: Dynamically scraping {url}...\")\n",
    "    \n",
    "    raw_text = fetch_and_clean_body(url)\n",
    "    clean_text = clean_text_content(raw_text)\n",
    "    \n",
    "    # Store in a different file as requested\n",
    "    file_path = \"live_contact_data.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"SOURCE: {url}\\n\\n{clean_text}\")\n",
    "        \n",
    "    return clean_text\n",
    "\n",
    "# --- 2. New Router Data Model ---\n",
    "\n",
    "class RouteDecision(BaseModel):\n",
    "    \"\"\"Router output model.\"\"\"\n",
    "    # CHANGE: Added \"conversational_node\" to the Literal list\n",
    "    destination: Literal[\"contact_retrieve_node\", \"retrieve_node\", \"conversational_node\"] = Field(\n",
    "        description=\"The next node. Choose 'contact_retrieve_node' for contact info about LMKR. Choose 'conversational_node' for greetings/chit-chat and information not LMKR and hence not requiring retrieval. Choose 'retrieve_node' for information about LMKR or its projects.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Nodes ---\n",
    "\n",
    "# NODE 1: RETRIEVE (With Augmentation)\n",
    "def retrieve_node(state: AgentState):\n",
    "    print(\"\\nðŸ” Node 1: Retrieve (Augmenting & Searching)...\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 1. Multi-Query Augmentation\n",
    "    parser = PydanticOutputParser(pydantic_object=QueryAugmentation)\n",
    "    prompt = f\"\"\"\n",
    "    User Question: {question}\n",
    "    Task: Generate 3 different search query variations to find relevant info in a corporate vector DB.\n",
    "    \"\"\"\n",
    "    structured_aug = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    queries = [question]\n",
    "    if structured_aug:\n",
    "        queries.extend(structured_aug.augmented_queries)\n",
    "        \n",
    "    # 2. Retrieve & Deduplicate\n",
    "    all_docs = []\n",
    "    for q in queries:\n",
    "        docs = retriever.invoke(q)\n",
    "        all_docs.extend([d.page_content for d in docs])\n",
    "    \n",
    "    unique_context = list(set(all_docs))[:5] # Limit to top 5 unique chunks\n",
    "\n",
    "    print (f\"   Retrieved {len(unique_context)} unique context chunks.\")\n",
    "    open(\"retrieved_context.txt\", \"w\", encoding=\"utf-8\").write(\"\".join(unique_context))\n",
    "    \n",
    "    return {\"context_chunks\": unique_context}\n",
    "\n",
    "# NODE 2: GENERATE (Safety & Context Focused)\n",
    "def generate_node(state: AgentState):\n",
    "    print(\"\\nâœï¸ Node 2: Generate Answer...\")\n",
    "    context = \"\\n---\\n\".join(state[\"context_chunks\"])\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=GeneratedAnswer)\n",
    "    prompt = f\"\"\"\n",
    "    Context Data:\n",
    "    {context}\n",
    "    \n",
    "    User Question: {question}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Answer the question using ONLY the provided Context Data.\n",
    "    2. If the answer is not in the context, state \"I do not have enough information.\"\n",
    "    3. Do not hallucinate or make up facts.\n",
    "    4. Maintain a professional tone.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_response = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback if generation fails\n",
    "    if not structured_response:\n",
    "        # CHANGE: sources_used must be a list now\n",
    "        structured_response = GeneratedAnswer(\n",
    "            answer=\"Error generating response.\", \n",
    "            sources_used=[\"None\"]\n",
    "        )\n",
    "        \n",
    "    return {\"generated_answer\": structured_response}\n",
    "\n",
    "# NODE 3: VALIDATE (Hallucination & Structure Check)\n",
    "def validate_node(state: AgentState):\n",
    "    print(\"\\nðŸ›¡ï¸ Node 3: Validate...\")\n",
    "    generation = state[\"generated_answer\"]\n",
    "    context = \"\\n---\\n\".join(state[\"context_chunks\"])\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=ValidationResult)\n",
    "    prompt = f\"\"\"\n",
    "    Context: {context}\n",
    "    Generated Answer: {generation.answer}\n",
    "    \n",
    "    Task: Validate the answer based on these strict rules:\n",
    "    1. Was the context actually used?\n",
    "    2. Are there any hallucinations (facts not in context)?\n",
    "    3. Is the structure correct?\n",
    "    4. If the answer is \"I do not have enough information.\", it is valid only if the context lacks relevant info.\n",
    "    \n",
    "    Return valid=True only if all checks pass. Otherwise return false\n",
    "    \"\"\"\n",
    "    \n",
    "    validation = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    if not validation:\n",
    "        # Default to fail if validator breaks\n",
    "        validation = ValidationResult(is_valid=False, reason=\"Validation process failed.\")\n",
    "        \n",
    "    return {\"validation\": validation}\n",
    "\n",
    "# NODE 4: ROUTER \n",
    "\n",
    "def router_node(state: AgentState):\n",
    "    print(\"\\nðŸš¦ Router: Analyzing User Intent...\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=RouteDecision)\n",
    "    prompt = f\"\"\"\n",
    "    User Question: {question}\n",
    "    \n",
    "    Role: You are a Router. \n",
    "    Task: Decide where to send this query.\n",
    "    \n",
    "    Rules:\n",
    "    1. If the user asks for Contact Info, Phone Numbers, Emails, Addresses, or Office Locations for LMKR, route to 'contact_retrieve_node'.\n",
    "    2. If the user uses greetings (Hi, Hello, Good Morning) or asks conversational questions (How are you?), route to 'conversational_node'.\n",
    "    3. For ANY other topic (Software, Geology, Company History, Solutions), route to 'retrieve_node'.\n",
    "    \"\"\"\n",
    "    \n",
    "    decision = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback default\n",
    "    if not decision:\n",
    "        return {\"destination\": \"retrieve_node\"}\n",
    "        \n",
    "    print(f\"   ðŸ‘‰ Routing to: {decision.destination}\")\n",
    "    return {\"destination\": decision.destination}\n",
    "\n",
    "# --- Split Nodes for Contact Flow ---\n",
    "\n",
    "# NODE: CONTACT RETRIEVE (Tool Execution Only)\n",
    "def contact_retrieve_node(state: AgentState):\n",
    "    print(\"\\nðŸ“ž Node: Contact Retrieve (Scraping)...\")\n",
    "    \n",
    "    \n",
    "    # 1. Run the Scrape Tool\n",
    "    contact_data = scrape_contact_tool()\n",
    "    \n",
    "    # 2. Update State\n",
    "    # We store the scraped text in 'context_chunks' so the next node can access it generically.\n",
    "    return {\"context_chunks\": [contact_data]}\n",
    "\n",
    "# NODE: CONTACT GENERATE (LLM Extraction Only)\n",
    "def contact_generate_node(state: AgentState):\n",
    "    print(\"\\nâœï¸ Node: Contact Generate (Extraction)...\")\n",
    "    \n",
    "    # 1. Get Inputs\n",
    "    question = state[\"question\"]\n",
    "    # We access the data we just scraped (it's in the first chunk)\n",
    "    contact_data = state[\"context_chunks\"][0] if state[\"context_chunks\"] else \"\"\n",
    "    \n",
    "    # 2. Prepare Prompt\n",
    "    parser = PydanticOutputParser(pydantic_object=GeneratedAnswer)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Live Scraped Data:\n",
    "    {contact_data}\n",
    "    \n",
    "    User Question: {question}\n",
    "    \n",
    "    Instructions:\n",
    "    Extract the contact details requested by the user from the scraped data above.\n",
    "    Format it clearly.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Call LLM\n",
    "    structured_response = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback\n",
    "    if not structured_response:\n",
    "        structured_response = GeneratedAnswer(\n",
    "            answer=\"I scraped the contact page but couldn't parse the details.\",\n",
    "            sources_used=[\"https://lmkr.com/contact/\"]\n",
    "        )\n",
    "        \n",
    "    return {\"generated_answer\": structured_response}\n",
    "\n",
    "# --- NEW NODE: CONVERSATIONAL ---\n",
    "def conversational_node(state: AgentState):\n",
    "    print(\"\\nðŸ’¬ Node: Conversational (Direct LLM)...\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=GeneratedAnswer)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    User Input: {question}\n",
    "    \n",
    "    Instructions:\n",
    "    1. You are a helpful corporate assistant for LMKR.\n",
    "    2. Respond naturally to the greeting or conversational question.\n",
    "    3. Do NOT make up technical facts. Just be polite.\n",
    "    4. Set 'sources_used' to [\"Conversational\"].\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_response = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback\n",
    "    if not structured_response:\n",
    "        structured_response = GeneratedAnswer(\n",
    "            answer=\"Hello! I am the LMKR AI Assistant. How can I help you with our software or services?\", \n",
    "            sources_used=[\"Conversational\"]\n",
    "        )\n",
    "        \n",
    "    # We return empty context_chunks to keep the state clean\n",
    "    return {\"generated_answer\": structured_response, \"context_chunks\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Graph & Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\trouter_node(router_node)\n",
      "\tcontact_retrieve_node(contact_retrieve_node)\n",
      "\tcontact_generate_node(contact_generate_node)\n",
      "\tretrieve_node(retrieve_node)\n",
      "\tgenerate_node(generate_node)\n",
      "\tvalidate_node(validate_node)\n",
      "\tconversational_node(conversational_node)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> router_node;\n",
      "\tcontact_generate_node --> validate_node;\n",
      "\tcontact_retrieve_node --> contact_generate_node;\n",
      "\tgenerate_node --> validate_node;\n",
      "\tretrieve_node --> generate_node;\n",
      "\trouter_node -.-> contact_retrieve_node;\n",
      "\trouter_node -.-> conversational_node;\n",
      "\trouter_node -.-> retrieve_node;\n",
      "\tvalidate_node -.-> __end__;\n",
      "\tvalidate_node -.-> contact_generate_node;\n",
      "\tvalidate_node -.-> generate_node;\n",
      "\tconversational_node --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Edge Logic ---\n",
    "\n",
    "def router(state: AgentState):\n",
    "    validation = state[\"validation\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # 1. If Valid, End\n",
    "    if validation and validation.is_valid:\n",
    "        print(\"âœ… Validation Passed.\")\n",
    "        return END\n",
    "    \n",
    "    # 2. If Max Retries, End\n",
    "    if retry_count >= 2:\n",
    "        print(\"ðŸ›‘ Max retries reached. Returning best effort.\")\n",
    "        return END\n",
    "        \n",
    "    # 3. Validation Failed - Decide where to loop back to\n",
    "    print(f\"ðŸ”„ Validation Failed: {validation.reason if validation else 'Unknown'}. Regenerating...\")\n",
    "    \n",
    "    # Check which path we were on\n",
    "    current_path = state.get(\"destination\")\n",
    "    \n",
    "    if current_path == \"contact_retrieve_node\":\n",
    "        return \"contact_generate_node\"\n",
    "    else:\n",
    "        return \"generate_node\" # Retry standard RAG generation\n",
    "\n",
    "# --- 7. Build Updated Graph ---\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# --- Add Nodes ---\n",
    "workflow.add_node(\"router_node\", router_node)\n",
    "workflow.add_node(\"contact_retrieve_node\", contact_retrieve_node)\n",
    "workflow.add_node(\"contact_generate_node\", contact_generate_node)\n",
    "workflow.add_node(\"retrieve_node\", retrieve_node)\n",
    "workflow.add_node(\"generate_node\", generate_node)\n",
    "workflow.add_node(\"validate_node\", validate_node)\n",
    "workflow.add_node(\"conversational_node\", conversational_node) # <--- ADDED\n",
    "\n",
    "# --- Set Entry Point ---\n",
    "workflow.set_entry_point(\"router_node\")\n",
    "\n",
    "# --- Routing Logic (Updated) ---\n",
    "workflow.add_conditional_edges(\n",
    "    \"router_node\",\n",
    "    lambda x: x[\"destination\"],\n",
    "    {\n",
    "        \"contact_retrieve_node\": \"contact_retrieve_node\",\n",
    "        \"retrieve_node\": \"retrieve_node\",\n",
    "        \"conversational_node\": \"conversational_node\" # <--- ADDED PATH\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Contact Path ---\n",
    "workflow.add_edge(\"contact_retrieve_node\", \"contact_generate_node\")\n",
    "workflow.add_edge(\"contact_generate_node\", \"validate_node\") \n",
    "\n",
    "# --- Standard RAG Path ---\n",
    "workflow.add_edge(\"retrieve_node\", \"generate_node\")\n",
    "workflow.add_edge(\"generate_node\", \"validate_node\")\n",
    "\n",
    "# --- Conversational Path (Short Circuit) ---\n",
    "# Greetings don't need validation or retrieval, so they go straight to END\n",
    "workflow.add_edge(\"conversational_node\", END) \n",
    "\n",
    "# --- Validation Logic ---\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate_node\",\n",
    "    router, \n",
    "    {\n",
    "        END: END,\n",
    "        \"contact_generate_node\": \"contact_generate_node\",\n",
    "        \"generate_node\": \"generate_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Visualize\n",
    "print(app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting RAG Pipeline (Retrieve -> Generate -> Validate)...\n",
      "\n",
      "ðŸš¦ Router: Analyzing User Intent...\n",
      "   ðŸ‘‰ Routing to: retrieve_node\n",
      "\n",
      "ðŸ” Node 1: Retrieve (Augmenting & Searching)...\n",
      "   Retrieved 5 unique context chunks.\n",
      "\n",
      "âœï¸ Node 2: Generate Answer...\n",
      "\n",
      "ðŸ›¡ï¸ Node 3: Validate...\n",
      "âœ… Validation Passed.\n",
      "\n",
      "ðŸŽ‰ Final Result:\n",
      "Answer: I am a company that provides professional services, including data management consulting and intelligent transport systems.\n",
      "Validation Status: Pass\n",
      "Source chunks Used: ['DATA MANAGEMENT', 'Intelligent Transport System - Providing Mobility Solutions To Transform Lives']\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Execution Test ---\n",
    "\n",
    "print(\"ðŸš€ Starting RAG Pipeline (Retrieve -> Generate -> Validate)...\")\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "initial_state = {\n",
    "    \"question\": \"who are you\", \n",
    "    \"retry_count\": 0,\n",
    "    \"context_chunks\": [],\n",
    "    \"generated_answer\": None,\n",
    "    \"validation\": None\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Final Result:\")\n",
    "if final_state.get('generated_answer'):\n",
    "    print(f\"Answer: {final_state['generated_answer'].answer}\")\n",
    "    \n",
    "    # FIX: Check if validation actually ran\n",
    "    if final_state.get('validation'):\n",
    "        status = 'Pass' if final_state['validation'].is_valid else 'Fail (Max Retries)'\n",
    "        print(f\"Validation Status: {status}\")\n",
    "    else:\n",
    "        print(\"Validation Status: N/A (Direct Contact Route)\")\n",
    "        \n",
    "    print(f\"Source chunks Used: {final_state['generated_answer'].sources_used}\")\n",
    "else:\n",
    "    print(\"Process failed to generate an answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
