{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F9tyhZxdfmZ"
      },
      "source": [
        "# LMKR RAG Chatbot - Jupyter Notebook\n",
        "\n",
        "This notebook implements a production-ready RAG (Retrieval-Augmented Generation) chatbot for LMKR using LangChain, HuggingFace LLM, and a custom vector database.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpZJ38pOdfma"
      },
      "source": [
        "## Cell 1: Project Setup & Dependencies Installation\n",
        "\n",
        "Install all required packages for LangChain, HuggingFace, FAISS, and utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDZf81Ozdfmb"
      },
      "outputs": [],
      "source": [
        "#!pip install -U langchain langchain-community langchain-huggingface langchain-text-splitters\n",
        "#!pip install -U huggingface-hub transformers torch\n",
        "#!pip install sentence-transformers hf_xet\n",
        "#!pip install faiss-cpu\n",
        "#!pip install python-dotenv\n",
        "#!pip install pypdf requests\n",
        "#!pip install accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geMb4v3edfmb"
      },
      "source": [
        "**What this cell does:**\n",
        "- Installs LangChain and integration libraries\n",
        "- Installs HuggingFace models and utilities\n",
        "- Sets up FAISS (Facebook AI Similarity Search) for vector database\n",
        "- Installs data loading utilities (PDF parsing, web scraping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXRQ32wTdfmb"
      },
      "source": [
        "## Cell 2: Imports & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QKS9_Tztdfmb"
      },
      "outputs": [],
      "source": [
        "# ---- Standard Libraries ----\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ---- HuggingFace (LangChain Integration) ----\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import (\n",
        "    HuggingFaceEmbeddings,\n",
        "    HuggingFacePipeline\n",
        ")\n",
        "\n",
        "# ---- LangChain Core ----\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# ---- Document Loaders & Vectorstores (Community packages) ----\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# ---- RAG / Retrieval (no RetrievalQA in v1.x, use LCEL) ----\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# ---- LLMs ----\n",
        "from langchain_community.llms import HuggingFaceHub  # if you use HF hub models\n",
        "# or:\n",
        "# from langchain_community.chat_models import ChatOpenAI, ChatAnthropic, etc.\n",
        "\n",
        "# ---- Utility ----\n",
        "import torch\n",
        "import json\n",
        "from typing import Dict, List, Dict\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PesZGEC6dfmc"
      },
      "source": [
        "**What this cell does:**\n",
        "- Imports all necessary libraries from LangChain, HuggingFace, and transformers\n",
        "- Sets up logging for debugging and monitoring\n",
        "- Loads environment variables (useful for API keys or configurations)\n",
        "- Prepares utilities for text splitting, embeddings, and vector storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O56_GZL8dfmc"
      },
      "source": [
        "## Cell 3: Configuration Parameters\n",
        "\n",
        "Centralized configuration for the entire RAG pipeline. Tune these to optimize chatbot performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAewiAhadfmc",
        "outputId": "60e0dc07-35d1-4a92-f8ca-d60817dfca08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Configuration loaded: LMKR-RAG-Chatbot\n",
            "âœ“ Device: Tesla T4\n",
            "âœ“ Results directory: ./results/20251127_081211\n"
          ]
        }
      ],
      "source": [
        "DATA_SOURCE_TYPE = \"text\"  # Options: \"text\", \"csv\", \"pdf\", \"url\"\n",
        "DATA_PATH = \"lmkr_combined.txt\"  # Path to your LMKR data file\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 500\n",
        "\n",
        "# 3. EMBEDDING MODEL CONFIG\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "MODEL_KWARGS = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
        "ENCODE_KWARGS = {\"normalize_embeddings\": False}\n",
        "\n",
        "# 4. VECTOR DATABASE CONFIG\n",
        "VECTOR_DB_PATH = \"./vector_db/faiss_lmkr\"\n",
        "USE_EXISTING_DB = False\n",
        "\n",
        "# 5. LLM MODEL CONFIG\n",
        "LLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "LLM_MAX_LENGTH = 100\n",
        "LLM_TEMPERATURE = 0.1\n",
        "LLM_TOP_P = 0.9\n",
        "\n",
        "\n",
        "\n",
        "SIMILARITY_THRESHOLD = 0.5  # Reject documents below this score\n",
        "RETRIEVER_K = 1  # Get top 2 documents for comparison\n",
        "MAX_HISTORY = 2  # Keep last 2 messages for context\n",
        "\n",
        "# 7. PROJECT METADATA\n",
        "PROJECT_NAME = \"LMKR-RAG-Chatbot\"\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_DIR = f\"./results/{TIMESTAMP}\"\n",
        "\n",
        "print(f\"âœ“ Configuration loaded: {PROJECT_NAME}\")\n",
        "print(f\"âœ“ Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"âœ“ Results directory: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWHkcNq1dfmc"
      },
      "source": [
        "**What this cell does:**\n",
        "- Centralizes all hyperparameters and configurations\n",
        "- Explains trade-offs for each parameter\n",
        "- Makes it easy to experiment with different models and settings\n",
        "- Provides alternative model suggestions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkaMk5j6dfmd"
      },
      "source": [
        "## Cell 4: Data Loading & Exploration\n",
        "\n",
        "Load your LMKR data from various sources (text, CSV, PDF, web)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNVb5iVVdfmd",
        "outputId": "e0f48679-6a76-4027-cc2f-ded07a7a6902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Successfully loaded LMKR data\n",
            "  - Total characters: 5,903\n",
            "  - Estimated tokens: 1,475\n",
            "  - Ready for RAG pipeline\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD LMKR DATA\n",
        "# Simple and clean - just raw text, no URLs\n",
        "# ============================================================\n",
        "\n",
        "try:\n",
        "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    print(f\"âœ“ Successfully loaded LMKR data\")\n",
        "    print(f\"  - Total characters: {len(raw_data):,}\")\n",
        "    print(f\"  - Estimated tokens: {len(raw_data) // 4:,}\")\n",
        "    print(f\"  - Ready for RAG pipeline\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ File not found: {DATA_PATH}\")\n",
        "    print(f\"Please ensure lmkr_combined.txt exists in ./data/lmkr_data/\")\n",
        "    raw_data = \"\"\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading file: {e}\")\n",
        "    raw_data = \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi5cqmiodfmd"
      },
      "source": [
        "**What this cell does:**\n",
        "- Loads LMKR data from various sources (text, CSV, PDF)\n",
        "- Handles different data formats automatically\n",
        "- Provides basic data statistics\n",
        "- Shows preview to verify data loaded correctly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkRO_HuSdfmd"
      },
      "source": [
        "## Cell 5: Data Preprocessing & Text Chunking\n",
        "\n",
        "Clean, normalize, and split text into manageable chunks for embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku8-W0Ekdfmd",
        "outputId": "f4285e2a-532b-424d-813a-7b2c22344247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text...\n",
            "Chunking text...\n",
            "\\nâœ“ Chunking Statistics:\n",
            "  - Total chunks: 11\n",
            "  - Average chunk size: 986 chars\n",
            "  - Max chunk size: 998 chars\n",
            "  - Min chunk size: 912 chars\n",
            "\\nFirst chunk preview:\\n# LMKR Company Data Collection & Storage Guide\n",
            "\n",
            "## ðŸ“Š COMPREHENSIVE LMKR COMPANY INFORMATION\n",
            "\n",
            "### **Company Overview**\n",
            "- **Name:** LMKR\n",
            "- **Founded:** 1994\n",
            "- **Headquarters:** Houston, Texas\n",
            "- **Type:** Privately Held Technology Company\n",
            "- **Employees:** 201-500\n",
            "- **Industry:** Oil & Gas, Geoscience, ...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PREPROCESS DATA & CREATE CHUNKS\n",
        "# This prepares raw text for embedding and retrieval\n",
        "# ============================================================\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean and preprocess text.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    text = re.sub(r'\\\\n+', '\\\\n', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into chunks using RecursiveCharacterTextSplitter.\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=overlap,\n",
        "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\n",
        "    )\n",
        "    chunks = splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# Execute preprocessing\n",
        "print(\"Preprocessing text...\")\n",
        "cleaned_data = preprocess_text(raw_data)\n",
        "\n",
        "print(\"Chunking text...\")\n",
        "text_chunks = chunk_text(cleaned_data, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "\n",
        "print(f\"\\\\nâœ“ Chunking Statistics:\")\n",
        "print(f\"  - Total chunks: {len(text_chunks)}\")\n",
        "if text_chunks:\n",
        "    print(f\"  - Average chunk size: {sum(len(c) for c in text_chunks) // len(text_chunks)} chars\")\n",
        "    print(f\"  - Max chunk size: {max(len(c) for c in text_chunks)} chars\")\n",
        "    print(f\"  - Min chunk size: {min(len(c) for c in text_chunks)} chars\")\n",
        "    print(f\"\\\\nFirst chunk preview:\\\\n{text_chunks[0][:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99VqB0O1dfmd"
      },
      "source": [
        "**What this cell does:**\n",
        "- Cleans and normalizes text\n",
        "- Splits text into manageable chunks (default 1000 chars)\n",
        "- Maintains overlap for context continuity\n",
        "- Provides statistics on chunking effectiveness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDOgy3Szdfmd"
      },
      "source": [
        "## Cell 6: Embedding Model Setup\n",
        "\n",
        "Initialize embedding model to convert text into vector representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG0l9T8Fdfme",
        "outputId": "c9fc9402-d396-49f4-b49c-82b730c10379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\nâœ“ Embedding test successful!\n",
            "  - Sample query embedding shape: 384\n",
            "  - First 5 values: [0.052791938185691833, 0.0005524340667761862, -0.05132013186812401, 0.07101751118898392, 0.03979623317718506]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# INITIALIZE EMBEDDING MODEL\n",
        "# This converts text into vector representations\n",
        "# ============================================================\n",
        "\n",
        "print(\"Loading embedding model...\")\n",
        "print(f\"Model: {EMBEDDING_MODEL_NAME}\")\n",
        "print(f\"Device: {MODEL_KWARGS['device']}\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs=MODEL_KWARGS,\n",
        "    encode_kwargs=ENCODE_KWARGS\n",
        ")\n",
        "\n",
        "# Test embedding on a sample\n",
        "test_text = \"LMKR is a test query\"\n",
        "test_embedding = embeddings.embed_query(test_text)\n",
        "print(f\"\\\\nâœ“ Embedding test successful!\")\n",
        "print(f\"  - Sample query embedding shape: {len(test_embedding)}\")\n",
        "print(f\"  - First 5 values: {test_embedding[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJZLFuWndfme"
      },
      "source": [
        "**What this cell does:**\n",
        "- Loads embedding model from HuggingFace\n",
        "- Initializes embeddings for your GPU/CPU\n",
        "- Tests embedding functionality\n",
        "- Explains model trade-offs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWo19pWBdfme"
      },
      "source": [
        "## Cell 7: Vector Database Creation & Indexing\n",
        "\n",
        "Convert text chunks to embeddings and store in FAISS for fast retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_BAO94kdfme",
        "outputId": "f03db7f5-0347-41c0-f4d4-c3c46b463f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vector database...\n",
            "Number of documents to embed: 11\n",
            "âœ“ Vector database created and saved to ./vector_db/faiss_lmkr\n",
            "\\n--- Testing Retriever ---\n",
            "Retrieved 1 documents for query: 'What is LMKR?'\n",
            "\\n[Document 1]\n",
            "Content: # LMKR Company Data Collection & Storage Guide\n",
            "\n",
            "## ðŸ“Š COMPREHENSIVE LMKR COMPANY INFORMATION\n",
            "\n",
            "### **Company Overview**\n",
            "- **Name:** LMKR\n",
            "- **Founded:** 1994\n",
            "- **Headquarters:** Houston, Texas\n",
            "- **Type:*...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CREATE & INDEX VECTOR DATABASE\n",
        "# Converts text chunks to embeddings and stores in FAISS\n",
        "# ============================================================\n",
        "\n",
        "def create_vector_db(documents: List[str], embeddings, db_path: str):\n",
        "    \"\"\"\n",
        "    Create FAISS vector database from documents.\n",
        "    \"\"\"\n",
        "    print(\"Creating vector database...\")\n",
        "    print(f\"Number of documents to embed: {len(documents)}\")\n",
        "\n",
        "    vector_db = FAISS.from_texts(\n",
        "        texts=documents,\n",
        "        embedding=embeddings,\n",
        "        metadatas=[{\"source\": f\"chunk_{i}\"} for i in range(len(documents))]\n",
        "    )\n",
        "\n",
        "    os.makedirs(db_path, exist_ok=True)\n",
        "    vector_db.save_local(db_path)\n",
        "    print(f\"âœ“ Vector database created and saved to {db_path}\")\n",
        "    return vector_db\n",
        "\n",
        "def load_vector_db(embeddings, db_path: str):\n",
        "    \"\"\"\n",
        "    Load existing FAISS vector database.\n",
        "    \"\"\"\n",
        "    print(f\"Loading vector database from {db_path}...\")\n",
        "    vector_db = FAISS.load_local(db_path, embeddings)\n",
        "    print(f\"âœ“ Vector database loaded successfully\")\n",
        "    return vector_db\n",
        "\n",
        "if USE_EXISTING_DB and os.path.exists(VECTOR_DB_PATH):\n",
        "    vector_store = load_vector_db(embeddings, VECTOR_DB_PATH)\n",
        "else:\n",
        "    vector_store = create_vector_db(text_chunks, embeddings, VECTOR_DB_PATH)\n",
        "\n",
        "print(\"\\\\n--- Testing Retriever ---\")\n",
        "test_query = \"What is LMKR?\"  # TODO: Modify based on domain\n",
        "retrieved_docs = vector_store.similarity_search(test_query, k=RETRIEVER_K)\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents for query: '{test_query}'\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\\\n[Document {i+1}]\")\n",
        "    print(f\"Content: {doc.page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlCyJ7TPdfme"
      },
      "source": [
        "**What this cell does:**\n",
        "- Converts all text chunks to embeddings\n",
        "- Creates FAISS index for fast similarity search\n",
        "- Saves index to disk for reuse\n",
        "- Tests retriever with a sample query\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J69JA0Y_dfme"
      },
      "source": [
        "## Cell 8: LLM Model Setup (HuggingFace)\n",
        "\n",
        "Load language model that will generate final responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "71e25b960a5c4476a0021c90c106fb49",
            "4bfe9d9d772346dfb856d2a675630b96",
            "489a868493a949878da7444bf37e5730",
            "de156baaabce4dae827b10ee6f856dbe",
            "c5b498f932b9467fbc9f84938cebf817",
            "a6b96aa40bb144b5b50041865959676a",
            "b55dbfaff6f84238ab0836e1db1616b8",
            "b7256da6f7d642e1a6403bb8cd71d08f",
            "59b6d3d5ee23452094f30d27531a3d34",
            "570b242d2e734b858b1efc2b3febd90e",
            "5b95f9d3a4f3408e90cdf60d154404ce"
          ]
        },
        "id": "ZsjJoXGmdfme",
        "outputId": "0b4048d8-bf82-46d7-de76-9992e7280f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLM from HuggingFace...\n",
            "Model: mistralai/Mistral-7B-Instruct-v0.1\n",
            "Device: Tesla T4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71e25b960a5c4476a0021c90c106fb49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model loaded successfully\n",
            "âœ“ LLM pipeline created successfully\n",
            "\\n--- Testing LLM ---\n",
            "Input: Tell me some of LMKR clients:\n",
            "Output: Tell me some of LMKR clients:\n",
            "\n",
            "1. A major oil and gas company in the Middle East\n",
            "2. A leading mining company in Australia\n",
            "3. A major construction company in the United States\n",
            "4. A global engineering a\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD LANGUAGE MODEL (HuggingFace)\n",
        "# This generates the final responses\n",
        "# ============================================================\n",
        "\n",
        "print(\"Loading LLM from HuggingFace...\")\n",
        "print(f\"Model: {LLM_MODEL_NAME}\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, max_length=LLM_MAX_LENGTH)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    )\n",
        "    print(\"âœ“ Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "# Create pipeline\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=LLM_MAX_LENGTH,\n",
        "    temperature=LLM_TEMPERATURE,\n",
        "    top_p=LLM_TOP_P,\n",
        ")\n",
        "\n",
        "# Wrap in LangChain\n",
        "llm = HuggingFacePipeline(\n",
        "    model_id=LLM_MODEL_NAME,\n",
        "    pipeline=llm_pipeline,\n",
        ")\n",
        "\n",
        "print(\"âœ“ LLM pipeline created successfully\")\n",
        "\n",
        "# Test LLM\n",
        "print(\"\\\\n--- Testing LLM ---\")\n",
        "test_prompt = \"Tell me some of LMKR clients:\"\n",
        "try:\n",
        "    test_output = llm.invoke(test_prompt)\n",
        "    print(f\"Input: {test_prompt}\")\n",
        "    print(f\"Output: {test_output[:200]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: LLM test skipped ({type(e).__name__})\")\n",
        "    print(f\"LLM is ready to use in RAG chain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uithjOL1dfme"
      },
      "source": [
        "**What this cell does:**\n",
        "- Loads tokenizer and model from HuggingFace\n",
        "- Configures GPU optimization\n",
        "- Creates text generation pipeline\n",
        "- Tests LLM functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJdEBdyYdfmf"
      },
      "source": [
        "## Cell 9: System Prompt Engineering\n",
        "\n",
        "Craft system prompts to guide chatbot behavior and response quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYSxmnqEdfmf",
        "outputId": "5386446c-7061-4ed5-d10c-bc0b23c79d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ System prompt configured for Mistral-7B\n",
            "âœ“ System prompts defined\n",
            "Active prompt: SYSTEM_PROMPT_CUSTOMER_SERVICE\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CRAFT SYSTEM PROMPTS FOR CHATBOT\n",
        "# This is critical for accuracy and response quality\n",
        "# ============================================================\n",
        "\n",
        "# Customer service oriented\n",
        "SYSTEM_PROMPT_TEMPLATE = \"\"\"Answer this question in your own words based on the provided information. write bla bla before answering.\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in 1-2 sentences:\"\"\"\n",
        "\n",
        "IMPROVED_SYSTEM_PROMPT = \"\"\"You are a helpful assistant answering questions about LMKR company.\n",
        "\n",
        "## CONTEXT FROM DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "## CONVERSATION HISTORY (Last 2 Exchanges):\n",
        "{history}\n",
        "\n",
        "## INSTRUCTIONS:\n",
        "1. Answer ONLY based on the provided context\n",
        "2. If the context doesn't contain relevant information, respond with: \"I don't have this information in my knowledge base.\"\n",
        "3. Keep answers concise (1-2 sentences)\n",
        "4. Be accurate and professional\n",
        "\n",
        "## USER QUESTION:\n",
        "{question}\n",
        "\n",
        "Answer in 1-2 sentences:\"\"\"\n",
        "\n",
        "ACTIVE_SYSTEM_PROMPT = SYSTEM_PROMPT_TEMPLATE\n",
        "\n",
        "print(\"âœ“ System prompt configured for Mistral-7B\")\n",
        "\n",
        "print(\"âœ“ System prompts defined\")\n",
        "print(f\"Active prompt: SYSTEM_PROMPT_CUSTOMER_SERVICE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d-wbkFhdfmf"
      },
      "source": [
        "**What this cell does:**\n",
        "- Defines multiple system prompt templates\n",
        "- Explains different prompt strategies\n",
        "- Provides template for context-aware responses\n",
        "\n",
        "**Prompt Engineering Tips:**\n",
        "- Be specific about role and constraints\n",
        "- Provide clear instructions on using context\n",
        "- Include examples of desired behavior\n",
        "- Experiment and measure quality differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-3HUVndfmf"
      },
      "source": [
        "## Cell 10: RAG Chain Assembly\n",
        "\n",
        "Combine retriever, prompt template, and LLM into complete RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoPQPGKFdfmf",
        "outputId": "c340b3fd-cd32-483f-e78e-14d757fae952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building RAG chain...\n",
            "âœ“ RAG chain assembled successfully\n",
            "Chain configuration:\n",
            "  - Retriever K: 1\n",
            "  - LLM: mistralai/Mistral-7B-Instruct-v0.1\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ASSEMBLE RAG CHAIN (MODERN LANGCHAIN APPROACH)\n",
        "# Combines retriever, prompt template, and LLM\n",
        "# ============================================================\n",
        "\n",
        "# Create prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=ACTIVE_SYSTEM_PROMPT\n",
        ")\n",
        "\n",
        "# Create retriever from vector store\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": RETRIEVER_K}\n",
        ")\n",
        "\n",
        "# Helper function to format documents\n",
        "def format_docs(docs):\n",
        "    \"\"\"Format retrieved documents for the prompt\"\"\"\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Assemble RAG chain using modern LCEL (LangChain Expression Language)\n",
        "print(\"Building RAG chain...\")\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"âœ“ RAG chain assembled successfully\")\n",
        "print(f\"Chain configuration:\")\n",
        "print(f\"  - Retriever K: {RETRIEVER_K}\")\n",
        "print(f\"  - LLM: {LLM_MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_retriever_with_threshold(vector_store, k: int, threshold: float):\n",
        "    \"\"\"\n",
        "    Create a retriever that filters by similarity threshold.\n",
        "    Returns both documents and their similarity scores.\n",
        "    \"\"\"\n",
        "    base_retriever = vector_store.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": k}\n",
        "    )\n",
        "    return base_retriever\n",
        "\n",
        "def get_relevant_docs_with_scores(retriever, query: str, threshold: float = 0.5):\n",
        "    \"\"\"\n",
        "    Get documents with similarity scores and filter by threshold.\n",
        "    \"\"\"\n",
        "    # Use similarity_search_with_scores for FAISS\n",
        "    try:\n",
        "        # Try new method\n",
        "        docs_with_scores = retriever.vectorstore.similarity_search_with_scores(query, k=RETRIEVER_K)\n",
        "\n",
        "        # Filter by threshold\n",
        "        relevant_docs = []\n",
        "        for doc, score in docs_with_scores:\n",
        "            # FAISS returns distance (lower is better), convert to similarity\n",
        "            # similarity = 1 / (1 + distance) or use 1 - normalized_distance\n",
        "            similarity = 1 / (1 + score)  # Convert distance to similarity [0, 1]\n",
        "            if similarity >= threshold:\n",
        "                relevant_docs.append((doc, similarity))\n",
        "\n",
        "        return relevant_docs\n",
        "    except:\n",
        "        # Fallback: use regular retriever\n",
        "        docs = retriever.invoke(query)\n",
        "        return [(doc, 1.0) for doc in docs]  # Assume max score if method fails\n",
        "\n",
        "# ============================================================\n",
        "# FORMAT HISTORY FOR PROMPT\n",
        "# ============================================================\n",
        "\n",
        "def format_history(history: list, max_items: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Format chat history for inclusion in prompt.\n",
        "    Keeps last N exchanges to maintain context.\n",
        "    \"\"\"\n",
        "    if not history:\n",
        "        return \"No previous context\"\n",
        "\n",
        "    # Get last max_items exchanges\n",
        "    recent_history = history[-max_items:]\n",
        "\n",
        "    formatted = []\n",
        "    for item in recent_history:\n",
        "        formatted.append(f\"User: {item['query']}\")\n",
        "        formatted.append(f\"Assistant: {item['answer']}\")\n",
        ""
      ],
      "metadata": {
        "id": "iBZydSbJTwKB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYPGKzBodfmf"
      },
      "source": [
        "**What this cell does:**\n",
        "- Creates prompt template for RAG\n",
        "- Converts vector store to retriever\n",
        "- Assembles RAG chain combining all components\n",
        "- Configures chain parameters\n",
        "\n",
        "**Chain Types:**\n",
        "- `stuff`: Concatenate docs (simple, works for small contexts)\n",
        "- `map_reduce`: Summarize each doc, then combine (for many docs)\n",
        "- `refine`: Iteratively improve answer (best accuracy, slower)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozy76mXDdfmf"
      },
      "source": [
        "## Cell 11: Testing & Inference\n",
        "\n",
        "Test RAG chain with sample queries and evaluate responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ318KeOdfmf",
        "outputId": "07112e51-7baa-474c-f414-1c78152b5d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TESTING RAG CHATBOT\n",
            "======================================================================\n",
            "\n",
            "[Query 1/4]\n",
            "Question: When was LMKR founded?\n",
            "----------------------------------------------------------------------\n",
            "Answer: LMKR was founded in 1994.\n",
            "\n",
            "[Query 2/4]\n",
            "Question: What is Gverse?\n",
            "----------------------------------------------------------------------\n",
            "Answer: GVERSE is a suite of geoscience interpretation products developed by LMKR. It includes GVERSE Geophysics, GVERSE Petrophysics, GVERSE Attributes, GVERSE Predict3D, GVERSE WebSteering, GVERSE Connect, GVERSE Planner, GVERSE Field Planner, GVERSE Inversion, and GVERSE GO.\n",
            "\n",
            "[Query 3/4]\n",
            "Question: What are LMKR's ISO-Certified Services?\n",
            "----------------------------------------------------------------------\n",
            "Answer: \"I don't have this information in my knowledge base.\"\n",
            "\n",
            "[Query 4/4]\n",
            "Question: Name one of LMKR's partnership?\n",
            "----------------------------------------------------------------------\n",
            "Answer: \"I don't have this information in my knowledge base.\"\n",
            "\n",
            "âœ“ Test results saved to ./results/20251127_081211/test_results.json\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# TEST RAG CHATBOT\n",
        "# Run queries and evaluate responses\n",
        "# ============================================================\n",
        "\n",
        "def query_chatbot_improved( rag_chain, retriever, query: str, history: list = None, similarity_threshold: float = SIMILARITY_THRESHOLD) -> Dict:\n",
        "    \"\"\"\n",
        "    Query the RAG chatbot with:\n",
        "    - Similarity threshold checking\n",
        "    - Conversation history context\n",
        "    - Confidence scoring\n",
        "    \"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Get documents with similarity scores\n",
        "    docs_with_scores = get_relevant_docs_with_scores(\n",
        "        retriever,\n",
        "        query,\n",
        "        threshold=similarity_threshold\n",
        "    )\n",
        "\n",
        "    # Check if we have relevant documents\n",
        "    if not docs_with_scores:\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": \"I don't have this information in my knowledge base.\",\n",
        "            \"source_documents\": [],\n",
        "            \"similarity_scores\": [],\n",
        "            \"confidence\": \"LOW\",\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    # Format documents and scores\n",
        "    docs = [doc for doc, score in docs_with_scores]\n",
        "    scores = [score for doc, score in docs_with_scores]\n",
        "\n",
        "    # Format context\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Format history\n",
        "    history_text = format_history(history, max_items=MAX_HISTORY)\n",
        "\n",
        "    # Create improved prompt with history\n",
        "    improved_prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\", \"history\"],\n",
        "        template=IMPROVED_SYSTEM_PROMPT\n",
        "    )\n",
        "\n",
        "    # Invoke chain with all context\n",
        "    try:\n",
        "        answer = improved_prompt.format(\n",
        "            context=context,\n",
        "            question=query,\n",
        "            history=history_text\n",
        "        )\n",
        "\n",
        "        # Get LLM response\n",
        "        llm_response = llm.invoke(answer)\n",
        "\n",
        "        # Clean up response\n",
        "        if \"Answer in 1-2 sentences:\" in llm_response:\n",
        "            llm_response = llm_response.split(\"Answer in 1-2 sentences:\")[-1].strip()\n",
        "\n",
        "        # Calculate confidence based on average similarity\n",
        "        avg_similarity = sum(scores) / len(scores) if scores else 0\n",
        "        confidence = \"HIGH\" if avg_similarity > 0.7 else \"MEDIUM\" if avg_similarity > 0.5 else \"LOW\"\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": llm_response,\n",
        "            \"source_documents\": docs,\n",
        "            \"similarity_scores\": scores,\n",
        "            \"avg_similarity\": avg_similarity,\n",
        "            \"confidence\": confidence,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": f\"I don't have this information in my knowledge base.\",\n",
        "            \"source_documents\": [],\n",
        "            \"similarity_scores\": [],\n",
        "            \"confidence\": \"LOW\",\n",
        "            \"error\": str(e),\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "\n",
        "test_queries = [\n",
        "    \"When was LMKR founded?\",\n",
        "    \"What is Gverse?\",\n",
        "    \"What are LMKR's ISO-Certified Services?\",\n",
        "    \"Name one of LMKR's partnership?\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING RAG CHATBOT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for i, query in enumerate(test_queries):\n",
        "    print(f\"\\n[Query {i+1}/{len(test_queries)}]\")\n",
        "    print(f\"Question: {query}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    try:\n",
        "        result = query_chatbot_improved(rag_chain, retriever, query)  # Pass retriever\n",
        "        test_results.append(result)\n",
        "\n",
        "        print(f\"Answer: {result['answer']}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing query: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "# Save test results\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "results_path = f\"{RESULTS_DIR}/test_results.json\"\n",
        "\n",
        "# Convert Document objects to dictionaries for JSON serialization\n",
        "serializable_results = []\n",
        "for result in test_results:\n",
        "    serializable_result = {\n",
        "        \"query\": result[\"query\"],\n",
        "        \"answer\": result[\"answer\"],\n",
        "        \"source_documents\": [\n",
        "            {\n",
        "                \"content\": doc.page_content,\n",
        "                \"metadata\": doc.metadata\n",
        "            }\n",
        "            for doc in result[\"source_documents\"]\n",
        "        ],\n",
        "        \"timestamp\": result[\"timestamp\"]\n",
        "    }\n",
        "    serializable_results.append(serializable_result)\n",
        "\n",
        "with open(results_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(serializable_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nâœ“ Test results saved to {results_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuxZ6XY2dfmg"
      },
      "source": [
        "**What this cell does:**\n",
        "- Tests RAG chain with sample queries\n",
        "- Captures answers and source documents\n",
        "- Shows retrieval effectiveness\n",
        "- Saves results for analysis\n",
        "\n",
        "**Next Steps After Testing:**\n",
        "- Evaluate answer quality\n",
        "- Check if correct documents were retrieved\n",
        "- Refine prompts if needed\n",
        "- Adjust retriever K if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF5Zinmadfmg"
      },
      "source": [
        "## Cell 12: Interactive Chat Loop\n",
        "\n",
        "Enable interactive chat with continuous conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Gcte64dfmg",
        "outputId": "ccd20389-3f02-430b-f70f-a18e97ff1c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LMKR RAG CHATBOT - Enhanced Mode with Memory\n",
            "Features: Conversation Memory + Confidence Scoring + Relevance Check\n",
            "Type 'quit' to exit, 'clear' to clear history\n",
            "======================================================================\n",
            "\n",
            "\n",
            "You: hi\n",
            "\n",
            "Bot: ## AI ASSISTANT:\n",
            "Hello! How can I assist you today?\n",
            "[Confidence: HIGH | Sources: 1]\n",
            "[Similarity Score: 1.00]\n",
            "\n",
            "\n",
            "You: when was lmkr founded\n",
            "\n",
            "Bot: LMKR was founded in 1994.\n",
            "[Confidence: HIGH | Sources: 1]\n",
            "[Similarity Score: 1.00]\n",
            "\n",
            "\n",
            "You: major partnerships of lmkr\n",
            "\n",
            "Bot: \"I don't have this information in my knowledge base.\"\n",
            "[Confidence: HIGH | Sources: 1]\n",
            "[Similarity Score: 1.00]\n",
            "\n",
            "\n",
            "You: iso certificaations\n",
            "\n",
            "Bot: LMKR offers ISO-certified services that transform complex E&P data into actionable knowledge, providing secure, transparent, and auditable data management.\n",
            "[Confidence: HIGH | Sources: 1]\n",
            "[Similarity Score: 1.00]\n",
            "\n",
            "\n",
            "You: partnerships\n",
            "\n",
            "Bot: 1. LMKR has partnerships with Halliburton Landmark, Asian Development Bank, GeoGraphix Suite, Contour Software, Devsinc, and Ignite - National Technology.\n",
            "2. These partnerships help LMKR provide technology advisory services, digital transformation services, software development consulting, AI-driven technology solutions, and complex problem-solving to clients in various industries such as Oil & Gas, Transportation, Agriculture, Energy,\n",
            "[Confidence: HIGH | Sources: 1]\n",
            "[Similarity Score: 1.00]\n",
            "\n",
            "\n",
            "You: quit\n",
            "\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "def chat_with_bot_improved(rag_chain, retriever, history: list = None, similarity_threshold: float = SIMILARITY_THRESHOLD):\n",
        "    \"\"\"\n",
        "    Interactive chat interface with:\n",
        "    - Memory of last 2 exchanges\n",
        "    - Similarity threshold checking\n",
        "    - Confidence scoring\n",
        "    \"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"LMKR RAG CHATBOT - Enhanced Mode with Memory\")\n",
        "    print(\"Features: Conversation Memory + Confidence Scoring + Relevance Check\")\n",
        "    print(\"Type 'quit' to exit, 'clear' to clear history\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\n\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() == \"quit\":\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input.lower() == \"clear\":\n",
        "            history = []\n",
        "            print(\"âœ“ History cleared\")\n",
        "            continue\n",
        "\n",
        "        if user_input.lower() == \"history\":\n",
        "            print(\"\\n--- Conversation History ---\")\n",
        "            if not history:\n",
        "                print(\"No history yet\")\n",
        "            else:\n",
        "                for i, exchange in enumerate(history[-MAX_HISTORY:], 1):\n",
        "                    print(f\"{i}. Q: {exchange['query']}\")\n",
        "                    print(f\"   A: {exchange['answer']}\")\n",
        "                    print(f\"   Confidence: {exchange.get('confidence', 'N/A')}\")\n",
        "            continue\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Query with memory and similarity threshold\n",
        "            result = query_chatbot_improved(\n",
        "                rag_chain,\n",
        "                retriever,\n",
        "                user_input,\n",
        "                history=history,\n",
        "                similarity_threshold=similarity_threshold\n",
        "            )\n",
        "\n",
        "            # Add to history\n",
        "            history.append(result)\n",
        "\n",
        "            # Display response\n",
        "            print(f\"\\nBot: {result['answer']}\")\n",
        "\n",
        "            # Show confidence and sources\n",
        "            print(f\"[Confidence: {result['confidence']} | Sources: {len(result['source_documents'])}]\")\n",
        "\n",
        "            if result['source_documents'] and result['avg_similarity']:\n",
        "                print(f\"[Similarity Score: {result['avg_similarity']:.2f}]\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "chat_history = chat_with_bot_improved(\n",
        "    rag_chain=rag_chain,\n",
        "    retriever=retriever,\n",
        "    similarity_threshold=0.5  # Adjust threshold as needed\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H26nFS0idfmg"
      },
      "source": [
        "**What this cell does:**\n",
        "- Provides interactive chat interface\n",
        "- Maintains conversation history\n",
        "- Shows bot responses in real-time\n",
        "\n",
        "**Usage:**\n",
        "- Uncomment the last line to activate\n",
        "- Type your questions\n",
        "- Type 'quit' to exit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2SmBMSjdfmg"
      },
      "source": [
        "## Cell 13: Evaluation & Metrics\n",
        "\n",
        "Measure and track chatbot quality metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHpC-kxkdfmg",
        "outputId": "2d59afb5-5f75-4a2c-c63b-e5abb2faf818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\n======================================================================\n",
            "EVALUATION METRICS\n",
            "======================================================================\n",
            "total_queries: 4\n",
            "avg_answer_length: 100\n",
            "avg_sources_retrieved: 1.0\n",
            "queries_with_sources: 4\n",
            "\\nâœ“ Metrics saved to ./results/20251127_081211/metrics.json\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# EVALUATE CHATBOT QUALITY\n",
        "# Measure accuracy, relevance, and response quality\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_responses(test_results: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate chatbot response quality.\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        \"total_queries\": len(test_results),\n",
        "        \"avg_answer_length\": 0,\n",
        "        \"avg_sources_retrieved\": 0,\n",
        "        \"queries_with_sources\": 0,\n",
        "    }\n",
        "\n",
        "    if not test_results:\n",
        "        return metrics\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_length = sum(len(r[\"answer\"]) for r in test_results)\n",
        "    total_sources = sum(len(r[\"source_documents\"]) for r in test_results)\n",
        "\n",
        "    metrics[\"avg_answer_length\"] = total_length // len(test_results)\n",
        "    metrics[\"avg_sources_retrieved\"] = total_sources / len(test_results)\n",
        "    metrics[\"queries_with_sources\"] = sum(\n",
        "        1 for r in test_results if len(r[\"source_documents\"]) > 0\n",
        "    )\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Evaluate test results\n",
        "if test_results:\n",
        "    metrics = evaluate_responses(test_results)\n",
        "    print(\"\\\\n\" + \"=\" * 70)\n",
        "    print(\"EVALUATION METRICS\")\n",
        "    print(\"=\" * 70)\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_path = f\"{RESULTS_DIR}/metrics.json\"\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    print(f\"\\\\nâœ“ Metrics saved to {metrics_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbtNJX1Jdfmk"
      },
      "source": [
        "**What this cell does:**\n",
        "- Calculates basic quality metrics\n",
        "- Provides framework for custom evaluation\n",
        "- Saves metrics for comparison\n",
        "\n",
        "**Evaluation Framework:**\n",
        "- Manual scoring: Rate 1-5 (relevance, accuracy, clarity)\n",
        "- Automated metrics: BLEU, ROUGE, similarity scores\n",
        "- Comparison: Test different models/prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA41TmIzdfml"
      },
      "source": [
        "## Cell 14: Save & Deploy Configuration\n",
        "\n",
        "Save all settings for reproducibility and future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0lCPltBdfml"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE PROJECT CONFIGURATION\n",
        "# Store settings for reproducibility\n",
        "# ============================================================\n",
        "\n",
        "def save_project_config():\n",
        "    \"\"\"\n",
        "    Save all project configuration to JSON.\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        \"project_name\": PROJECT_NAME,\n",
        "        \"timestamp\": TIMESTAMP,\n",
        "        \"data\": {\n",
        "            \"source_type\": DATA_SOURCE_TYPE,\n",
        "            \"path\": DATA_PATH,\n",
        "            \"num_chunks\": len(text_chunks) if text_chunks else 0,\n",
        "            \"chunk_size\": CHUNK_SIZE,\n",
        "            \"chunk_overlap\": CHUNK_OVERLAP,\n",
        "        },\n",
        "        \"embedding\": {\n",
        "            \"model\": EMBEDDING_MODEL_NAME,\n",
        "            \"device\": MODEL_KWARGS[\"device\"],\n",
        "        },\n",
        "        \"vectordb\": {\n",
        "            \"path\": VECTOR_DB_PATH,\n",
        "            \"retriever_k\": RETRIEVER_K,\n",
        "        },\n",
        "        \"llm\": {\n",
        "            \"model\": LLM_MODEL_NAME,\n",
        "            \"max_length\": LLM_MAX_LENGTH,\n",
        "            \"temperature\": LLM_TEMPERATURE,\n",
        "            \"top_p\": LLM_TOP_P,\n",
        "        },\n",
        "        \"rag_chain\": {\n",
        "            \"chain_type\": CHAIN_TYPE,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    config_path = f\"{RESULTS_DIR}/config.json\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"âœ“ Configuration saved to {config_path}\")\n",
        "    return config\n",
        "\n",
        "# Save configuration\n",
        "config = save_project_config()\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Project: {config['project_name']}\")\n",
        "print(f\"Timestamp: {config['timestamp']}\")\n",
        "print(f\"Results directory: {RESULTS_DIR}\")\n",
        "print(f\"\\\\nKey Components:\")\n",
        "print(f\"  - Data chunks: {config['data']['num_chunks']}\")\n",
        "print(f\"  - Embedding model: {config['embedding']['model']}\")\n",
        "print(f\"  - LLM: {config['llm']['model']}\")\n",
        "print(f\"  - Retriever K: {config['vectordb']['retriever_k']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGdHVNaydfml"
      },
      "source": [
        "**What this cell does:**\n",
        "- Saves all configurations for reproducibility\n",
        "- Documents experiment settings\n",
        "- Creates project summary\n",
        "\n",
        "**Why It Matters:**\n",
        "- Enables reproducible experiments\n",
        "- Documents what settings worked best\n",
        "- Useful for sharing with teammates\n",
        "- Easy to restart/continue work"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71e25b960a5c4476a0021c90c106fb49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bfe9d9d772346dfb856d2a675630b96",
              "IPY_MODEL_489a868493a949878da7444bf37e5730",
              "IPY_MODEL_de156baaabce4dae827b10ee6f856dbe"
            ],
            "layout": "IPY_MODEL_c5b498f932b9467fbc9f84938cebf817"
          }
        },
        "4bfe9d9d772346dfb856d2a675630b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6b96aa40bb144b5b50041865959676a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b55dbfaff6f84238ab0836e1db1616b8",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "489a868493a949878da7444bf37e5730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7256da6f7d642e1a6403bb8cd71d08f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59b6d3d5ee23452094f30d27531a3d34",
            "value": 2
          }
        },
        "de156baaabce4dae827b10ee6f856dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570b242d2e734b858b1efc2b3febd90e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5b95f9d3a4f3408e90cdf60d154404ce",
            "value": "â€‡2/2â€‡[01:03&lt;00:00,â€‡29.46s/it]"
          }
        },
        "c5b498f932b9467fbc9f84938cebf817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6b96aa40bb144b5b50041865959676a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b55dbfaff6f84238ab0836e1db1616b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7256da6f7d642e1a6403bb8cd71d08f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b6d3d5ee23452094f30d27531a3d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "570b242d2e734b858b1efc2b3febd90e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b95f9d3a4f3408e90cdf60d154404ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}