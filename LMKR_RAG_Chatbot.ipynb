{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Data Structures\n",
    "We define strict Pydantic models for every node's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Optional, TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Load Your Existing Resources ---\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Load Vector DB\n",
    "VECTOR_DB_PATH = \"./vector_db/faiss_lmkr\"\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "except:\n",
    "    # Fallback for testing\n",
    "    print(\"âš ï¸ DB not found, creating dummy.\")\n",
    "    vectorstore = FAISS.from_texts([\"LMKR founded in 1994.\"], embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "# LLM Client\n",
    "hf_client = InferenceClient(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    token=os.getenv(\"HF_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# --- 2. Define Structured Outputs (Pydantic) ---\n",
    "\n",
    "class SearchPlan(BaseModel):\n",
    "    \"\"\"Output for Thinking Bot\"\"\"\n",
    "    reasoning: str = Field(description=\"Why we need to search this\")\n",
    "    search_queries: List[str] = Field(description=\"List of 1-3 optimized search queries\")\n",
    "\n",
    "class ActionResponse(BaseModel):\n",
    "    \"\"\"Output for Action Bot\"\"\"\n",
    "    answer: str = Field(description=\"The final answer to the user\")\n",
    "    used_context: str = Field(description=\"The specific context snippets used\")\n",
    "\n",
    "class EvaluationReport(BaseModel):\n",
    "    \"\"\"Output for Evaluation Bot\"\"\"\n",
    "    score: int = Field(description=\"Score from 1-10\")\n",
    "    is_grounded: bool = Field(description=\"True if answer is supported by context\")\n",
    "    explanation: str = Field(description=\"Reason for the score\")\n",
    "    \n",
    "# --- 3. Define Graph State ---\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    plan: Optional[SearchPlan]\n",
    "    response: Optional[ActionResponse]\n",
    "    evaluation: Optional[EvaluationReport]\n",
    "    retry_count: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Helper for Mistral JSON Enforcement\n",
    "Since Mistral can be chatty, this helper ensures we get clean JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_structured(prompt_text: str, parser: PydanticOutputParser) -> BaseModel:\n",
    "    \"\"\"\n",
    "    Wraps HF Client to force Pydantic output.\n",
    "    \"\"\"\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    \n",
    "    final_prompt = f\"\"\"{prompt_text}\n",
    "    \n",
    "    IMPORTANT: You must output ONLY a valid JSON object.\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate\n",
    "    response = hf_client.text_generation(\n",
    "        final_prompt, \n",
    "        max_new_tokens=500, \n",
    "        temperature=0.1 # Low temp for structure\n",
    "    )\n",
    "    \n",
    "    # Clean and Parse\n",
    "    try:\n",
    "        # sometimes models add text before the json\n",
    "        json_str = response.strip()\n",
    "        if \"```json\" in json_str:\n",
    "            json_str = json_str.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"{\" in json_str:\n",
    "            json_str = json_str[json_str.find(\"{\"):json_str.rfind(\"}\")+1]\n",
    "            \n",
    "        return parser.parse(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JSON Parsing Failed: {e}\")\n",
    "        print(f\"Raw Output: {response}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define The 3 Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NODE 1: THINKING BOT ---\n",
    "def thinking_node(state: AgentState):\n",
    "    print(\"\\nðŸ§  Thinking Bot: Analyzing request...\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=SearchPlan)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    User Question: {question}\n",
    "    \n",
    "    Task: Break this down into search queries for a vector database containing LMKR company data.\n",
    "    If the question is simple, just generate one query.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_output = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback if parsing fails\n",
    "    if not structured_output:\n",
    "        structured_output = SearchPlan(reasoning=\"Parse failed\", search_queries=[question])\n",
    "        \n",
    "    return {\"plan\": structured_output}\n",
    "\n",
    "\n",
    "# --- NODE 2: ACTION BOT ---\n",
    "def action_node(state: AgentState):\n",
    "    print(\"\\nâš¡ Action Bot: Executing plan...\")\n",
    "    plan = state[\"plan\"]\n",
    "    \n",
    "    # 1. Retrieve Data based on plan\n",
    "    all_docs = []\n",
    "    for query in plan.search_queries:\n",
    "        docs = retriever.invoke(query)\n",
    "        all_docs.extend(docs)\n",
    "    \n",
    "    # Deduplicate and Format\n",
    "    unique_content = list(set([d.page_content for d in all_docs]))\n",
    "    context_str = \"\\n---\\n\".join(unique_content[:3]) # Limit context size\n",
    "    \n",
    "    # 2. Generate Answer\n",
    "    parser = PydanticOutputParser(pydantic_object=ActionResponse)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Context: {context_str}\n",
    "    \n",
    "    User Question: {state['question']}\n",
    "    \n",
    "    Task: Answer the question using ONLY the context provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_output = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    return {\"response\": structured_output}\n",
    "\n",
    "\n",
    "# --- NODE 3: EVALUATION BOT ---\n",
    "def evaluation_node(state: AgentState):\n",
    "    print(\"\\nâš–ï¸ Evaluation Bot: Grading answer...\")\n",
    "    response = state[\"response\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=EvaluationReport)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Generated Answer: {response.answer}\n",
    "    Context Used: {response.used_context}\n",
    "    \n",
    "    Task: Evaluate if the answer is grounded in the context and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_output = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    return {\"evaluation\": structured_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Graph & Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logic for Conditional Edge ---\n",
    "def route_after_eval(state: AgentState):\n",
    "    \"\"\"\n",
    "    Decide: End or Retry?\n",
    "    \"\"\"\n",
    "    evaluation = state[\"evaluation\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    if evaluation and evaluation.is_grounded:\n",
    "        print(\"âœ… Answer Approved\")\n",
    "        return END\n",
    "    \n",
    "    if retry_count > 2:\n",
    "        print(\"ðŸ›‘ Max retries reached. Stopping.\")\n",
    "        return END\n",
    "        \n",
    "    print(\"ðŸ”„ Answer Rejected. Retrying Thinking...\")\n",
    "    return \"thinking_bot\"\n",
    "\n",
    "# --- Graph Construction ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"thinking_bot\", thinking_node)\n",
    "workflow.add_node(\"action_bot\", action_node)\n",
    "workflow.add_node(\"evaluation_bot\", evaluation_node)\n",
    "\n",
    "# Add Edges\n",
    "workflow.set_entry_point(\"thinking_bot\")\n",
    "workflow.add_edge(\"thinking_bot\", \"action_bot\")\n",
    "workflow.add_edge(\"action_bot\", \"evaluation_bot\")\n",
    "\n",
    "# Conditional Edge from Evaluation\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluation_bot\",\n",
    "    route_after_eval,\n",
    "    {\n",
    "        END: END,\n",
    "        \"thinking_bot\": \"thinking_bot\"\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test it\n",
    "initial_state = {\n",
    "    \"question\": \"What is LMKR's history?\",\n",
    "    \"retry_count\": 0\n",
    "}\n",
    "\n",
    "print(\"ðŸš€ Starting Structured LangGraph...\")\n",
    "for output in app.stream(initial_state):\n",
    "    pass # Streaming is handled by print statements inside nodes\n",
    "\n",
    "# Access final state manually if needed (not shown in stream loop for simplicity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
