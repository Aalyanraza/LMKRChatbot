{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Data Structures\n",
    "We define strict Pydantic models for every node's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Optional, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup & Configuration (Kept Exact) ---\n",
    "load_dotenv()\n",
    "\n",
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Vector DB\n",
    "VECTOR_DB_PATH = \"./vector_db/faiss_lmkr\"\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "except:\n",
    "    print(\"âš ï¸ DB not found, creating dummy for execution safety.\")\n",
    "    vectorstore = FAISS.from_texts([\"LMKR founded in 1994. GVERSE is a software brand.\"], embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "# LLM Client\n",
    "hf_client = InferenceClient(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    token=os.getenv(\"HF_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# --- 2. Pydantic Models for Structured Output ---\n",
    "\n",
    "class QueryAugmentation(BaseModel):\n",
    "    \"\"\"Output for Node 1: Retrieval Augmentation\"\"\"\n",
    "    augmented_queries: List[str] = Field(\n",
    "        description=\"List of 3 alternative versions of the user question to improve search coverage.\"\n",
    "    )\n",
    "\n",
    "class GeneratedAnswer(BaseModel):\n",
    "    \"\"\"Output for Node 2: Generation\"\"\"\n",
    "    answer: str = Field(description=\"The response to the user.\")\n",
    "    # FIX: Add default=[] so it doesn't crash if the model forgets this field\n",
    "    sources_used: List[str] = Field(default=[], description=\"List of context chunks or titles used.\")\n",
    "\n",
    "    # FIX: Add a validator to try and rescue the sources if they are stuck inside the answer text\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def rescue_sources(cls, data):\n",
    "        # If data is just a string (sometimes happens), wrap it\n",
    "        if isinstance(data, str):\n",
    "            return {\"answer\": data, \"sources_used\": []}\n",
    "            \n",
    "        # If 'sources_used' is missing but 'answer' mentions them, try to clean up\n",
    "        if isinstance(data, dict):\n",
    "            answer_text = data.get(\"answer\", \"\")\n",
    "            if \"Sources Used:\" in answer_text and \"sources_used\" not in data:\n",
    "                # Basic cleanup to separate them (optional, but nice to have)\n",
    "                parts = answer_text.split(\"Sources Used:\")\n",
    "                data[\"answer\"] = parts[0].strip()\n",
    "                # We won't try too hard to parse the list string, just prevent the crash\n",
    "                data[\"sources_used\"] = [\"Mentioned in answer\"]\n",
    "                \n",
    "        return data\n",
    "\n",
    "    @field_validator('answer', mode='before')\n",
    "    @classmethod\n",
    "    def flatten_list_answer(cls, v):\n",
    "        if isinstance(v, list):\n",
    "            return \", \".join(map(str, v))\n",
    "        return v\n",
    "        \n",
    "class ValidationResult(BaseModel):\n",
    "    \"\"\"Output for Node 3: Validation\"\"\"\n",
    "    is_valid: bool = Field(description=\"True if context was used correctly and no hallucinations found.\")\n",
    "    reason: str = Field(description=\"Explanation of validation failure or success.\")\n",
    "\n",
    "# --- 3. State Definition ---\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    context_chunks: List[str]\n",
    "    generated_answer: Optional[GeneratedAnswer]\n",
    "    validation: Optional[ValidationResult]\n",
    "    retry_count: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Helper for Mistral JSON Enforcement\n",
    "Since Mistral can be chatty, this helper ensures we get clean JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_structured(prompt_text: str, parser: PydanticOutputParser) -> Optional[BaseModel]:\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    \n",
    "    # FIX 1: Stronger prompt to stop it from returning the Schema Definition\n",
    "    final_prompt = f\"\"\"{prompt_text}\n",
    "    \n",
    "    IMPORTANT INSTRUCTIONS:\n",
    "    1. Output ONLY a valid JSON object. \n",
    "    2. Do NOT output the schema definition or \"properties\" block. Output the actual data instance.\n",
    "    3. Do NOT escape underscores (e.g., use \"sources_used\", NOT \"sources\\\\_used\").\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": final_prompt}]\n",
    "        response = hf_client.chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        json_str = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean Markdown wrapping\n",
    "        if \"```json\" in json_str:\n",
    "            json_str = json_str.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in json_str:\n",
    "            json_str = json_str.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "        # FIX 2: Manually fix the \"sources\\_used\" error common in Mistral models\n",
    "        json_str = json_str.replace(r\"\\_\", \"_\")\n",
    "\n",
    "        # FIX 3: Detect if model returned a Schema instead of Data (Node 1 Fix)\n",
    "        # If the JSON looks like {\"properties\": {...}, \"type\": \"object\"}, it failed.\n",
    "        # We can try to salvage it or just return None to trigger a retry/fallback.\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            if \"properties\" in data and \"type\" in data and data.get(\"type\") == \"object\":\n",
    "                print(\"âš ï¸ Model returned schema instead of data. Retrying parse...\")\n",
    "                # Sometimes models put the answer inside 'default' or 'example' fields of the schema, \n",
    "                # but usually it's best to just fail and let the fallback handle it.\n",
    "                return None\n",
    "        except:\n",
    "            pass # Not valid JSON yet, let the parser handle the error\n",
    "            \n",
    "        return parser.parse(json_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JSON Parsing/API Failed: {e}\")\n",
    "        # print(f\"Raw: {json_str}\") # Uncomment for debugging\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define The Router and The 5 Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Literal\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# --- 1. Tool Extraction from scraping.py ---\n",
    "\n",
    "def fetch_and_clean_body(url: str, depth=0) -> str:\n",
    "    if depth > 1: return \"\"\n",
    "    print(f\"   ðŸ–¥ï¸ Booting Headless Edge for: {url}\")\n",
    "    \n",
    "    edge_options = EdgeOptions()\n",
    "    edge_options.add_argument(\"--headless\")\n",
    "    edge_options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    # FIX 1: Initialize variable to None OUTSIDE the try block\n",
    "    driver = None \n",
    "    \n",
    "    try:\n",
    "        # FIX 2: Remove 'webdriver_manager'. Selenium 4.6+ downloads drivers automatically.\n",
    "        # This fixes the \"Could not reach host\" error in many cases.\n",
    "        driver = webdriver.Edge(options=edge_options)\n",
    "        \n",
    "        driver.get(url)\n",
    "        time.sleep(5) \n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Cleanup tags\n",
    "        for tag in soup([\"nav\", \"footer\", \"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "            tag.decompose()\n",
    "            \n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            return body.get_text(separator=\"\\n\")\n",
    "        else:\n",
    "            return soup.get_text(separator=\"\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Selenium Error: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        # FIX 3: Check if driver exists before quitting\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "def clean_text_content(text: str) -> str:\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    NOISE_PHRASES = [\"warning\", \"required\", \"skip to content\", \"all rights reserved\"]\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if len(stripped) < 3: continue\n",
    "        if any(phrase in stripped.lower() for phrase in NOISE_PHRASES): continue\n",
    "        cleaned_lines.append(stripped)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "@tool\n",
    "def scrape_contact_tool():\n",
    "    \"\"\"\n",
    "    Scrapes the official LMKR contact page (https://lmkr.com/contact/) \n",
    "    to retrieve live addresses, phone numbers, and emails.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://lmkr.bamboohr.com/careers\"\n",
    "    print(f\"ðŸ•¸ï¸ Tool Triggered: Dynamically scraping {url}...\")\n",
    "    \n",
    "    raw_text = fetch_and_clean_body(url)\n",
    "    clean_text = clean_text_content(raw_text)\n",
    "    \n",
    "    file_path = \"live_contact_data.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"SOURCE: {url}\\n\\n{clean_text}\")\n",
    "        \n",
    "    return clean_text\n",
    "\n",
    "@tool\n",
    "def scrape_careers_tool():\n",
    "    \"\"\"\n",
    "    Scrapes the official LMKR careers page (https://lmkr.com/careers/) \n",
    "    to retrieve live job openings, requirements, and application emails.\n",
    "    \"\"\"\n",
    "    # Target the careers page\n",
    "    url = \"https://lmkr.bamboohr.com/careers\"\n",
    "    print(f\"ðŸ•¸ï¸ Tool Triggered: Dynamically scraping {url}...\")\n",
    "    \n",
    "    # Use your existing helper functions\n",
    "    raw_text = fetch_and_clean_body(url)\n",
    "    clean_text = clean_text_content(raw_text)\n",
    "\n",
    "    file_path = \"live_careers_data.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"SOURCE: {url}\\n\\n{clean_text}\")\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "@tool\n",
    "def scrape_news_fast_tool():\n",
    "    \"\"\"\n",
    "    Scrapes the LMKR announcements page (https://lmkr.com/announcements) \n",
    "    using Requests + BS4 to retrieve the latest news and press releases.\n",
    "    \"\"\"\n",
    "    url = \"https://lmkr.com/announcements\"\n",
    "    print(f\"ðŸ—žï¸ Tool Triggered: Fast scraping {url}...\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() \n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Cleanup irrelevant tags\n",
    "        for tag in soup([\"nav\", \"footer\", \"script\", \"style\", \"noscript\", \"svg\", \"header\"]):\n",
    "            tag.decompose()\n",
    "            \n",
    "        body = soup.find('body')\n",
    "        # Using the clean_text_content helper from your earlier cell\n",
    "        clean_text = clean_text_content(body.get_text(separator=\"\\n\")) if body else \"\"\n",
    "        \n",
    "        # Save to file\n",
    "        file_path = \"live_news_data.txt\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"SOURCE: {url}\\n\\n{clean_text}\")\n",
    "            \n",
    "        return clean_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fast Scrape Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. New Router Data Model ---\n",
    "\n",
    "class RouteDecision(BaseModel):\n",
    "    \"\"\"Router output model.\"\"\"\n",
    "    destination: Literal[\"career_retrieve_node\", \"retrieve_node\", \"news_retrieve_node\", \"conversational_node\"] = Field(\n",
    "        description=\"Choose 'news_retrieve_node' for announcements, press releases, or latest news about LMKR. Choose 'career_retrieve_node' for jobs/vacancies. Choose 'conversational_node' for chat. Choose 'retrieve_node' for everything else.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Information Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Nodes ---\n",
    "\n",
    "def retrieve_node(state: AgentState):\n",
    "    print(\"\\nðŸ” Node 1: Retrieve (Augmenting & Searching)...\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # --- ADAPTIVE LOGIC ---\n",
    "    # Check if we are retrying. If so, widen the search scope.\n",
    "    current_retry = state.get(\"retry_count\", 0)\n",
    "    base_k = 5\n",
    "    \n",
    "    # Increase k by 3 for every retry (e.g., 5 -> 8 -> 11)\n",
    "    dynamic_k = base_k + (current_retry * 3) \n",
    "    \n",
    "    if current_retry > 0:\n",
    "        print(f\"   ðŸ”„ Retry #{current_retry} detected: Expanding search context to top-{dynamic_k} chunks.\")\n",
    "    \n",
    "    # 1. Multi-Query Augmentation\n",
    "    parser = PydanticOutputParser(pydantic_object=QueryAugmentation)\n",
    "    prompt = f\"\"\"\n",
    "    User Question: {question}\n",
    "    Task: Generate 3 different search query variations to find relevant info in a corporate vector DB.\n",
    "    \"\"\"\n",
    "    structured_aug = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    queries = [question]\n",
    "    if structured_aug:\n",
    "        queries.extend(structured_aug.augmented_queries)\n",
    "        \n",
    "    # 2. Retrieve & Deduplicate\n",
    "    all_docs = []\n",
    "    for q in queries:\n",
    "        # UPDATED: Use vectorstore directly to enforce the dynamic 'k'\n",
    "        # (retriever.invoke uses the fixed k=5 set at initialization)\n",
    "        docs = vectorstore.similarity_search(q, k=dynamic_k)\n",
    "        all_docs.extend([d.page_content for d in docs])\n",
    "    \n",
    "    # UPDATED: Slice using dynamic_k, not hardcoded [:5]\n",
    "    # We use set() to remove exact duplicates from overlapping queries\n",
    "    unique_context = list(set(all_docs))[:dynamic_k] \n",
    "\n",
    "    print (f\"   Retrieved {len(unique_context)} unique context chunks (Target: {dynamic_k}).\")\n",
    "    \n",
    "    # Debug log\n",
    "    open(\"retrieved_context.txt\", \"w\", encoding=\"utf-8\").write(\"\".join(unique_context))\n",
    "    \n",
    "    return {\"context_chunks\": unique_context}\n",
    "\n",
    "# NODE 2: GENERATE (Safety & Context Focused)\n",
    "def generate_node(state: AgentState):\n",
    "    print(\"\\nâœï¸ Node: Generate (Unified)...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    # Join the context chunks from EITHER retrieval path\n",
    "    context_data = \"\\n---\\n\".join(state[\"context_chunks\"])\n",
    "    \n",
    "    if not context_data:\n",
    "        context_data = \"No information found in the retrieved context.\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=GeneratedAnswer)\n",
    "    \n",
    "    # --- UNIFIED PROMPT ---\n",
    "    # This prompt works for both General QA and Job Listings\n",
    "    prompt = f\"\"\"\n",
    "    Context Data:\n",
    "    {context_data}\n",
    "    \n",
    "    User Question: {question}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Answer the user's question using ONLY the provided Context Data.\n",
    "    2. If the context contains a list of items (like job openings, software features, or locations), present them clearly as a list.\n",
    "    3. If the answer is not in the context, state \"I do not have enough information.\"\n",
    "    4. Do not hallucinate. Maintain a professional tone.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_response = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback if generation fails\n",
    "    if not structured_response:\n",
    "        structured_response = GeneratedAnswer(\n",
    "            answer=\"Error generating response.\", \n",
    "            sources_used=[\"None\"]\n",
    "        )\n",
    "        \n",
    "    return {\n",
    "        \"generated_answer\": structured_response, \n",
    "        \"retry_count\": state.get(\"retry_count\", 0) + 1\n",
    "    }\n",
    "# NODE 3: VALIDATE (Hallucination & Structure Check)\n",
    "def validate_node(state: AgentState):\n",
    "    print(\"\\nðŸ›¡ï¸ Node 3: Robust Validation...\")\n",
    "    \n",
    "    # Unpack state\n",
    "    generation = state[\"generated_answer\"]\n",
    "    context_chunks = state[\"context_chunks\"]\n",
    "    context_text = \"\\n---\\n\".join(context_chunks)\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 1. Immediate Pass for Conversational/Fallbacks\n",
    "    # If the answer acknowledges no info, we accept it (it's honest, not a hallucination)\n",
    "    if \"I do not have enough information\" in generation.answer:\n",
    "        return {\n",
    "            \"validation\": ValidationResult(is_valid=True, reason=\"Honest fallback triggered.\")\n",
    "        }\n",
    "        \n",
    "    if \"Conversational\" in generation.sources_used:\n",
    "         return {\n",
    "            \"validation\": ValidationResult(is_valid=True, reason=\"Conversational turn.\")\n",
    "        }\n",
    "\n",
    "    # 2. Stronger Validation Prompt\n",
    "    parser = PydanticOutputParser(pydantic_object=ValidationResult)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a strict Quality Control Auditor.\n",
    "    \n",
    "    User Question: {question}\n",
    "    Generated Answer: {generation.answer}\n",
    "    \n",
    "    Reference Context:\n",
    "    {context_text}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Break the Generated Answer into individual claims.\n",
    "    2. For EACH claim, attempt to find a supporting quote in the Reference Context.\n",
    "    3. If a claim exists in the Answer but NOT in the Context, it is a HALLUCINATION.\n",
    "    4. Ignore minor phrasing differences; look for semantic meaning.\n",
    "    \n",
    "    Output JSON:\n",
    "    - set 'is_valid' to false if ANY unsupported claim is found.\n",
    "    - set 'reason' to a specific explanation of what fact was unsupported.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reuse your existing helper\n",
    "    validation = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    if not validation:\n",
    "        # If validator crashes, assume unsafe and force retry\n",
    "        validation = ValidationResult(is_valid=False, reason=\"Validation LLM failed to parse.\")\n",
    "        \n",
    "    print(f\"   Evaluation: {'âœ… PASS' if validation.is_valid else 'âŒ FAIL'} | Reason: {validation.reason}\")\n",
    "    \n",
    "    return {\"validation\": validation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE 4: ROUTER \n",
    "\n",
    "def router_node(state: AgentState):\n",
    "    print(\"\\nðŸš¦ Router: Analyzing User Intent...\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=RouteDecision)\n",
    "    prompt = f\"\"\"\n",
    "    User Question: {question}\n",
    "    \n",
    "    Role: You are a Router. \n",
    "    Task: Decide where to send this query.\n",
    "    \n",
    "    Rules:\n",
    "    1. If the user asks about News, Announcements, Press Releases, or Recent Updates about LMKR, route to 'news_retrieve_node'.\n",
    "    2. If the user asks about Jobs, Careers, Vacancies, Internships about LMKR, route to 'career_retrieve_node'.\n",
    "    3. If the user uses greetings (Hi, Hello) or generic chat or anything not related to LMKR, route to 'conversational_node'.\n",
    "    4. For everything else (Company History, Software info, Contact, Services, Products), route to 'retrieve_node'.\n",
    "    \"\"\"\n",
    "    \n",
    "    decision = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Default fallback\n",
    "    if not decision:\n",
    "        return {\"destination\": \"retrieve_node\"}\n",
    "        \n",
    "    print(f\"   ðŸ‘‰ Routing to: {decision.destination}\")\n",
    "    return {\"destination\": decision.destination}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW NODE: CONVERSATIONAL ---\n",
    "def conversational_node(state: AgentState):\n",
    "    print(\"\\nðŸ’¬ Node: Conversational (Direct LLM)...\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=GeneratedAnswer)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    User Input: {question}\n",
    "    \n",
    "    Instructions:\n",
    "    1. You are a helpful corporate assistant for LMKR.\n",
    "    2. Respond naturally to the greeting or conversational question.\n",
    "    3. Do NOT make up technical facts. Just be polite.\n",
    "    4. Set 'sources_used' to [\"Conversational\"].\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_response = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback\n",
    "    if not structured_response:\n",
    "        structured_response = GeneratedAnswer(\n",
    "            answer=\"Hello! I am the LMKR AI Assistant. How can I help you with our software or services?\", \n",
    "            sources_used=[\"Conversational\"]\n",
    "        )\n",
    "        \n",
    "    # We return empty context_chunks to keep the state clean\n",
    "    return {\"generated_answer\": structured_response, \"context_chunks\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Career Information Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NODE 1: CAREER RETRIEVE ---\n",
    "def career_retrieve_node(state: AgentState):\n",
    "    print(\"\\nðŸ’¼ Node: Career Retrieve (Adaptive)...\")\n",
    "    question = state[\"question\"]\n",
    "    current_retry = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # ADAPTIVE LOGIC: Widen search on retries\n",
    "    base_k = 4\n",
    "    dynamic_k = base_k + (current_retry * 4) # 4 -> 8 -> 12\n",
    "    \n",
    "    # We need a persistent way to hold the vectorstore between retries.\n",
    "    # In a stateless graph, we usually rebuild it from the raw text in 'state' or re-scrape.\n",
    "    # To keep this simple and robust:\n",
    "    \n",
    "    # 1. CHECK FOR EXISTING CONTEXT (Avoid Re-scraping if possible)\n",
    "    # If we are retrying, we might want to rely on previously scraped text if available.\n",
    "    # However, since 'context_chunks' holds the *selected* chunks, we likely need the full raw text.\n",
    "    # For this implementation, we will re-run the tool if it's the first run, \n",
    "    # but strictly rely on a broader search if we are looping back.\n",
    "    \n",
    "    raw_text = \"\"\n",
    "    \n",
    "    # Optimization: You could store 'full_scraped_text' in AgentState to avoid calling the tool again.\n",
    "    # For now, we will call the tool only if we don't have a cache mechanism, \n",
    "    # but typically you don't want to re-scrape in a loop.\n",
    "    \n",
    "    # Simulating a check: If we are in a retry loop, we assume the previous scrape was valid \n",
    "    # but we missed the relevant chunk. \n",
    "    # NOTE: Since the tool writes to a file, we can read that file on retry instead of hitting the web.\n",
    "    \n",
    "    if current_retry > 0 and os.path.exists(\"live_careers_data.txt\"):\n",
    "        print(f\"   ðŸ”„ Retry #{current_retry}: Reading cached career data (Skipping Web Scrape)...\")\n",
    "        with open(\"live_careers_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "    else:\n",
    "        # First run or file missing -> Active Scrape\n",
    "        raw_text = scrape_careers_tool.invoke({})\n",
    "\n",
    "    if not raw_text:\n",
    "        print(\"   âš ï¸ Warning: Scrape returned empty data.\")\n",
    "        return {\"context_chunks\": []}\n",
    "\n",
    "    # 2. Chunk & Index (Re-building index is fast for small text)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_text(raw_text)\n",
    "    \n",
    "    temp_vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "    \n",
    "    # 3. Dynamic Search\n",
    "    print(f\"   Searching career data with k={dynamic_k}...\")\n",
    "    retrieved_docs = temp_vectorstore.similarity_search(question, k=dynamic_k)\n",
    "    retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    print(f\"   Retrieved {len(retrieved_texts)} relevant career chunks.\")\n",
    "    return {\"context_chunks\": retrieved_texts}\n",
    "\n",
    "# --- NODE 2: CAREER GENERATE ---\n",
    "def career_generate_node(state: AgentState):\n",
    "    print(\"\\nâœï¸ Node: Career Generate...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    context_data = \"\\n---\\n\".join(state[\"context_chunks\"])\n",
    "    \n",
    "    if not context_data:\n",
    "        context_data = \"No specific job openings found.\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=GeneratedAnswer)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Context (Live Job Board Data):\n",
    "    {context_data}\n",
    "    \n",
    "    User Question: {question}\n",
    "    \n",
    "    Instructions:\n",
    "    1. ANALYZE the Context Data first. Does it contain specific job titles (e.g., \"Software Engineer\", \"Geophysicist\")?\n",
    "    2. If the Context Data only contains generic company info (\"rewarding place to work\", \"benefits\") but NO specific job titles, you MUST output: \"I could not retrieve the live job list at this time.\"\n",
    "    3. If valid jobs are listed, answer the user's question.\n",
    "    4. WARNING: Do not invent job titles. Do not list jobs that are not explicitly in the text above.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_response = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback\n",
    "    if not structured_response:\n",
    "        structured_response = GeneratedAnswer(\n",
    "            answer=\"I checked the careers page but couldn't parse the listings.\", \n",
    "            sources_used=[\"https://lmkr.com/careers/\"]\n",
    "        )\n",
    "        \n",
    "    current_retries = state.get(\"retry_count\", 0)\n",
    "    return {\"generated_answer\": structured_response, \"retry_count\": current_retries + 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News/ Announcements Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW NODE: NEWS RETRIEVE ---\n",
    "def news_retrieve_node(state: AgentState):\n",
    "    print(\"\\nðŸ—žï¸ Node: News Retrieve (Fast & Adaptive)...\")\n",
    "    question = state[\"question\"]\n",
    "    current_retry = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # Adaptive Logic: Widen search on retries\n",
    "    base_k = 4\n",
    "    dynamic_k = base_k + (current_retry * 4) \n",
    "    \n",
    "    raw_text = \"\"\n",
    "    \n",
    "    # Check cache to avoid re-requesting if looping\n",
    "    if current_retry > 0 and os.path.exists(\"live_news_data.txt\"):\n",
    "        print(f\"   ðŸ”„ Retry #{current_retry}: Reading cached news data...\")\n",
    "        with open(\"live_news_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "    else:\n",
    "        # First run -> Active Fast Scrape\n",
    "        raw_text = scrape_news_fast_tool.invoke({})\n",
    "\n",
    "    if not raw_text:\n",
    "        print(\"   âš ï¸ Warning: News scrape returned empty data.\")\n",
    "        return {\"context_chunks\": []}\n",
    "\n",
    "    # Chunk & Index into Temporary Vector Store\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_text(raw_text)\n",
    "    \n",
    "    if not chunks:\n",
    "         return {\"context_chunks\": []}\n",
    "         \n",
    "    temp_vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "    \n",
    "    # Search\n",
    "    print(f\"   Searching news data with k={dynamic_k}...\")\n",
    "    retrieved_docs = temp_vectorstore.similarity_search(question, k=dynamic_k)\n",
    "    retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    print(f\"   Retrieved {len(retrieved_texts)} relevant news chunks.\")\n",
    "    \n",
    "    # Return chunks to the SHARED state (so Generate Node can use them)\n",
    "    return {\"context_chunks\": retrieved_texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Graph & Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Edge Logic ---\n",
    "\n",
    "def router(state: AgentState):\n",
    "    validation = state[\"validation\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # 1. If Valid, End\n",
    "    if validation and validation.is_valid:\n",
    "        print(\"âœ… Validation Passed.\")\n",
    "        return END\n",
    "    \n",
    "    # 2. If Max Retries, End\n",
    "    if retry_count >= 2:\n",
    "        print(\"ðŸ›‘ Max retries reached. Returning best effort.\")\n",
    "        return END\n",
    "        \n",
    "    # 3. Validation Failed - Decide where to loop back to\n",
    "    print(f\"ðŸ”„ Validation Failed: {validation.reason if validation else 'Unknown'}. Regenerating...\")\n",
    "    \n",
    "    # Check which path we were on\n",
    "    current_path = state.get(\"destination\")\n",
    "    \n",
    "    if current_path == \"contact_retrieve_node\":\n",
    "        return \"contact_generate_node\"\n",
    "    else:\n",
    "        return \"generate_node\" # Retry standard RAG generation\n",
    "\n",
    "def validation_router(state: AgentState):\n",
    "    validation = state[\"validation\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    # We need to know where we came from to know where to go back to\n",
    "    destination = state.get(\"destination\", \"retrieve_node\") \n",
    "\n",
    "    # 1. Success\n",
    "    if validation and validation.is_valid:\n",
    "        return END\n",
    "    \n",
    "    # 2. Max Retries\n",
    "    if retry_count >= 2:\n",
    "        print(\"ðŸ›‘ Max retries reached. Returning best effort.\")\n",
    "        return END\n",
    "        \n",
    "    # 3. FAILURE -> LOOP BACK\n",
    "    print(f\"ðŸ”„ Validation Failed: {validation.reason}. Expanding search context...\")\n",
    "    \n",
    "    if destination == \"career_retrieve_node\":\n",
    "        return \"career_retrieve_node\"\n",
    "    elif destination == \"news_retrieve_node\":   # <--- HANDLE NEWS LOOP\n",
    "        return \"news_retrieve_node\"\n",
    "    elif destination == \"conversational_node\":\n",
    "        return \"conversational_node\"\n",
    "    else:\n",
    "        return \"retrieve_node\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\trouter_node(router_node)\n",
      "\tcareer_retrieve_node(career_retrieve_node)\n",
      "\tnews_retrieve_node(news_retrieve_node)\n",
      "\tretrieve_node(retrieve_node)\n",
      "\tconversational_node(conversational_node)\n",
      "\tgenerate_node(generate_node)\n",
      "\tvalidate_node(validate_node)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> router_node;\n",
      "\tcareer_retrieve_node --> generate_node;\n",
      "\tgenerate_node --> validate_node;\n",
      "\tnews_retrieve_node --> generate_node;\n",
      "\tretrieve_node --> generate_node;\n",
      "\trouter_node -.-> career_retrieve_node;\n",
      "\trouter_node -.-> conversational_node;\n",
      "\trouter_node -.-> news_retrieve_node;\n",
      "\trouter_node -.-> retrieve_node;\n",
      "\tvalidate_node -.-> __end__;\n",
      "\tvalidate_node -.-> career_retrieve_node;\n",
      "\tvalidate_node -.-> conversational_node;\n",
      "\tvalidate_node -.-> news_retrieve_node;\n",
      "\tvalidate_node -.-> retrieve_node;\n",
      "\tconversational_node --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"router_node\", router_node)\n",
    "workflow.add_node(\"career_retrieve_node\", career_retrieve_node)\n",
    "workflow.add_node(\"news_retrieve_node\", news_retrieve_node) # <--- ADD NODE\n",
    "workflow.add_node(\"retrieve_node\", retrieve_node)\n",
    "workflow.add_node(\"conversational_node\", conversational_node)\n",
    "workflow.add_node(\"generate_node\", generate_node) \n",
    "workflow.add_node(\"validate_node\", validate_node)\n",
    "\n",
    "# Entry\n",
    "workflow.set_entry_point(\"router_node\")\n",
    "\n",
    "# Conditional Edges from Router\n",
    "workflow.add_conditional_edges(\n",
    "    \"router_node\",\n",
    "    lambda x: x[\"destination\"],\n",
    "    {\n",
    "        \"career_retrieve_node\": \"career_retrieve_node\",\n",
    "        \"news_retrieve_node\": \"news_retrieve_node\", # <--- ADD EDGE\n",
    "        \"retrieve_node\": \"retrieve_node\",\n",
    "        \"conversational_node\": \"conversational_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Connect Retrieval Nodes to Generator\n",
    "workflow.add_edge(\"career_retrieve_node\", \"generate_node\")\n",
    "workflow.add_edge(\"news_retrieve_node\", \"generate_node\") # <--- ADD EDGE\n",
    "workflow.add_edge(\"retrieve_node\", \"generate_node\")\n",
    "\n",
    "# Generator -> Validator\n",
    "workflow.add_edge(\"generate_node\", \"validate_node\")\n",
    "\n",
    "# Conditional Edges from Validator (The Loop)\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate_node\",\n",
    "    validation_router, \n",
    "    {\n",
    "        END: END,\n",
    "        \"retrieve_node\": \"retrieve_node\",             \n",
    "        \"career_retrieve_node\": \"career_retrieve_node\", \n",
    "        \"news_retrieve_node\": \"news_retrieve_node\", # <--- ADD LOOP BACK\n",
    "        \"conversational_node\": \"conversational_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"conversational_node\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "# Visualize\n",
    "print(app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting RAG Pipeline (Retrieve -> Generate -> Validate)...\n",
      "\n",
      "ðŸš¦ Router: Analyzing User Intent...\n",
      "   ðŸ‘‰ Routing to: news_retrieve_node\n",
      "\n",
      "ðŸ—žï¸ Node: News Retrieve (Fast & Adaptive)...\n",
      "ðŸ—žï¸ Tool Triggered: Fast scraping https://lmkr.com/announcements...\n",
      "   Searching news data with k=4...\n",
      "   Retrieved 4 relevant news chunks.\n",
      "\n",
      "âœï¸ Node: Generate (Unified)...\n",
      "\n",
      "ðŸ›¡ï¸ Node 3: Robust Validation...\n",
      "   Evaluation: âœ… PASS | Reason: LMKR achieved ISO 9001 and ISO 27001 certification across USA, UAE, and Pakistan as stated in the context.\n",
      "LMKR strengthened cloud relationship with Oracle as stated in the context.\n",
      "LMKR exhibited at SPE & PAPGâ€™s Annual Technical Conference & Oil Show and ADIPEC Exhibition and Conference as stated in the context.\n",
      "LMKR signed agreements to modernize property tax systems and land record management systems in various locations as stated in the context.\n",
      "\n",
      " Query:  Latest news about LMKR\n",
      "\n",
      "ðŸŽ‰ Final Result:\n",
      "Answer: LMKR has recently achieved ISO 9001 and ISO 27001 certification across the USA, UAE, and Pakistan. They have also strengthened their cloud relationship with Oracle and signed agreements to modernize property tax systems and land record management systems in various locations. LMKR also exhibited at conferences and townhalls.\n",
      "Validation Status: Pass\n",
      "Source chunks Used: ['LMKR Achieves ISO 9001 & ISO 27001 Certification Across USA, UAE & Pakistan', 'LMKR, Oracle Strengthen Cloud Relationship', 'LMK 2023 Townhall: A Year of Expanding Horizons', 'Annual Technical Conference & Oil Show', 'ADIPEC Exhibition and Conference', 'LMKR, Oracle Strengthen Cloud Relationship (Dubai)', 'LMKR Partners with LG&HTP to Modernize Karachiâ€™s Property Tax System', 'LMKR, PULSE Partner to Transform Punjabâ€™s Land Record Systems with Next-Gen LRMIS', 'LMKR Empowers GEPCO to Revolutionize Distribution Network Management with Advanced GIS Solution']\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Execution Test ---\n",
    "\n",
    "print(\"ðŸš€ Starting RAG Pipeline (Retrieve -> Generate -> Validate)...\")\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "initial_state = {\n",
    "    \"question\": \"Latest news about LMKR\", \n",
    "    \"retry_count\": 0,\n",
    "    \"context_chunks\": [],\n",
    "    \"generated_answer\": None,\n",
    "    \"validation\": None\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n Query: \", initial_state[\"question\"])\n",
    "print(\"\\nðŸŽ‰ Final Result:\")\n",
    "if final_state.get('generated_answer'):\n",
    "    print(f\"Answer: {final_state['generated_answer'].answer}\")\n",
    "    \n",
    "    # FIX: Check if validation actually ran\n",
    "    if final_state.get('validation'):\n",
    "        status = 'Pass' if final_state['validation'].is_valid else 'Fail (Max Retries)'\n",
    "        print(f\"Validation Status: {status}\")\n",
    "    else:\n",
    "        print(\"Validation Status: N/A (Direct Contact Route)\")\n",
    "        \n",
    "    print(f\"Source chunks Used: {final_state['generated_answer'].sources_used}\")\n",
    "else:\n",
    "    print(\"Process failed to generate an answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
