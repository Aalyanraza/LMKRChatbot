{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Data Structures\n",
    "We define strict Pydantic models for every node's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Optional, TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# --- 1. Load Your Existing Resources ---\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Load Vector DB\n",
    "VECTOR_DB_PATH = \"./vector_db/faiss_lmkr\"\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "except:\n",
    "    # Fallback for testing\n",
    "    print(\"‚ö†Ô∏è DB not found, creating dummy.\")\n",
    "    vectorstore = FAISS.from_texts([\"LMKR founded in 1994.\"], embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "# LLM Client\n",
    "hf_client = InferenceClient(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    token=os.getenv(\"HF_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# --- 2. Define Structured Outputs (Pydantic) ---\n",
    "\n",
    "class SearchPlan(BaseModel):\n",
    "    \"\"\"Output for Thinking Bot\"\"\"\n",
    "    reasoning: str = Field(description=\"Why we need to search this\")\n",
    "    search_queries: List[str] = Field(description=\"List of 1-3 optimized search queries\")\n",
    "\n",
    "class ActionResponse(BaseModel):\n",
    "    \"\"\"Output for Action Bot\"\"\"\n",
    "    answer: str = Field(description=\"The final answer to the user\")\n",
    "    used_context: str = Field(description=\"The specific context snippets used\")\n",
    "\n",
    "class EvaluationReport(BaseModel):\n",
    "    \"\"\"Output for Evaluation Bot\"\"\"\n",
    "    score: int = Field(description=\"Score from 1-10\")\n",
    "    is_grounded: bool = Field(description=\"True if answer is supported by context\")\n",
    "    explanation: str = Field(description=\"Reason for the score\")\n",
    "    \n",
    "# --- 3. Define Graph State ---\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    plan: Optional[SearchPlan]\n",
    "    response: Optional[ActionResponse]\n",
    "    evaluation: Optional[EvaluationReport]\n",
    "    retry_count: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Helper for Mistral JSON Enforcement\n",
    "Since Mistral can be chatty, this helper ensures we get clean JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_structured(prompt_text: str, parser: PydanticOutputParser) -> Optional[BaseModel]:\n",
    "    \"\"\"\n",
    "    Wraps HF Client to force Pydantic output using Chat Completion API.\n",
    "    \"\"\"\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    \n",
    "    # 1. Construct the prompt with strict JSON instructions\n",
    "    final_prompt = f\"\"\"{prompt_text}\n",
    "    \n",
    "    IMPORTANT: You must output ONLY a valid JSON object. Do not add markdown formatting or conversational filler.\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2. CHANGE: Use chat_completion instead of text_generation\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": final_prompt}]\n",
    "        \n",
    "        response = hf_client.chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=500,       # Note: Parameter is 'max_tokens' for chat, not 'max_new_tokens'\n",
    "            temperature=0.1       # Low temp for structure\n",
    "        )\n",
    "        \n",
    "        # 3. Extract the actual text content from the Chat response object\n",
    "        json_str = response.choices[0].message.content.strip()\n",
    "\n",
    "        # 4. Clean Markdown if present (Common issue with Mistral)\n",
    "        if \"```json\" in json_str:\n",
    "            json_str = json_str.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in json_str:\n",
    "            json_str = json_str.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        # 5. Parse\n",
    "        return parser.parse(json_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå JSON Parsing/API Failed: {e}\")\n",
    "        # print(f\"Raw Output: {json_str}\") # Uncomment to debug raw string\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define The 3 Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NODE 1: THINKING BOT ---\n",
    "def thinking_node(state: AgentState):\n",
    "    print(\"\\nüß† Thinking Bot: Analyzing request...\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=SearchPlan)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    User Question: {question}\n",
    "    \n",
    "    Task: Break this down into search queries for a vector database containing LMKR company data.\n",
    "    If the question is simple, just generate one query.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_output = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    # Fallback if parsing fails\n",
    "    if not structured_output:\n",
    "        structured_output = SearchPlan(reasoning=\"Parse failed\", search_queries=[question])\n",
    "        \n",
    "    return {\"plan\": structured_output}\n",
    "\n",
    "\n",
    "# --- NODE 2: ACTION BOT ---\n",
    "def action_node(state: AgentState):\n",
    "    print(\"\\n‚ö° Action Bot: Executing plan...\")\n",
    "    plan = state[\"plan\"]\n",
    "    \n",
    "    # 1. Retrieve Data based on plan\n",
    "    all_docs = []\n",
    "    for query in plan.search_queries:\n",
    "        docs = retriever.invoke(query)\n",
    "        all_docs.extend(docs)\n",
    "    \n",
    "    # Deduplicate and Format\n",
    "    unique_content = list(set([d.page_content for d in all_docs]))\n",
    "    context_str = \"\\n---\\n\".join(unique_content[:3]) # Limit context size\n",
    "    \n",
    "    # 2. Generate Answer\n",
    "    parser = PydanticOutputParser(pydantic_object=ActionResponse)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Context: {context_str}\n",
    "    \n",
    "    User Question: {state['question']}\n",
    "    \n",
    "    Task: Answer the question using ONLY the context provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_output = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    return {\"response\": structured_output}\n",
    "\n",
    "\n",
    "# --- NODE 3: EVALUATION BOT ---\n",
    "def evaluation_node(state: AgentState):\n",
    "    print(\"\\n‚öñÔ∏è Evaluation Bot: Grading answer...\")\n",
    "    response = state[\"response\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=EvaluationReport)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Generated Answer: {response.answer}\n",
    "    Context Used: {response.used_context}\n",
    "    \n",
    "    Task: Evaluate if the answer is grounded in the context and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_output = query_llm_structured(prompt, parser)\n",
    "    \n",
    "    return {\"evaluation\": structured_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Graph & Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logic for Conditional Edge ---\n",
    "def route_after_eval(state: AgentState):\n",
    "    \"\"\"\n",
    "    Decide: End or Retry?\n",
    "    \"\"\"\n",
    "    evaluation = state[\"evaluation\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    if evaluation and evaluation.is_grounded:\n",
    "        print(\"‚úÖ Answer Approved\")\n",
    "        return END\n",
    "    \n",
    "    if retry_count > 2:\n",
    "        print(\"üõë Max retries reached. Stopping.\")\n",
    "        return END\n",
    "        \n",
    "    print(\"üîÑ Answer Rejected. Retrying Thinking...\")\n",
    "    return \"thinking_bot\"\n",
    "\n",
    "# --- Graph Construction ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"thinking_bot\", thinking_node)\n",
    "workflow.add_node(\"action_bot\", action_node)\n",
    "workflow.add_node(\"evaluation_bot\", evaluation_node)\n",
    "\n",
    "# Add Edges\n",
    "workflow.set_entry_point(\"thinking_bot\")\n",
    "workflow.add_edge(\"thinking_bot\", \"action_bot\")\n",
    "workflow.add_edge(\"action_bot\", \"evaluation_bot\")\n",
    "\n",
    "# Conditional Edge from Evaluation\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluation_bot\",\n",
    "    route_after_eval,\n",
    "    {\n",
    "        END: END,\n",
    "        \"thinking_bot\": \"thinking_bot\"\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Structured LangGraph...\n",
      "\n",
      "üß† Thinking Bot: Analyzing request...\n",
      "\n",
      "‚ö° Action Bot: Executing plan...\n",
      "\n",
      "‚öñÔ∏è Evaluation Bot: Grading answer...\n",
      "‚úÖ Answer Approved\n",
      "\n",
      "üéâ Final Output:\n",
      "Answer: GVERSE is a geoscience software solution offered by LMKR.\n",
      "Evaluation: 10/10\n",
      "Reasoning: The answer 'GVERSE is a geoscience software solution offered by LMKR' is directly stated in the context 'GVERSE GeoGraphix Release 2026.1'.\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "initial_state = {\n",
    "    \"question\": \"What is gverse?\",\n",
    "    \"retry_count\": 0\n",
    "}\n",
    "\n",
    "print(\"üöÄ Starting Structured LangGraph...\")\n",
    "\n",
    "# CHANGE: Use .invoke() to get the complete final state dictionary\n",
    "# The print statements inside your nodes will still show up!\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "# Now we can safely access all parts of the state\n",
    "print(\"\\nüéâ Final Output:\")\n",
    "\n",
    "if final_state.get('response'):\n",
    "    print(f\"Answer: {final_state['response'].answer}\")\n",
    "else:\n",
    "    print(\"Answer: N/A\")\n",
    "\n",
    "if final_state.get('evaluation'):\n",
    "    print(f\"Evaluation: {final_state['evaluation'].score}/10\")\n",
    "    print(f\"Reasoning: {final_state['evaluation'].explanation}\")\n",
    "else:\n",
    "    print(\"Evaluation: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
